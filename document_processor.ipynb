{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8871e9d0-be54-460e-b656-8315543e7669",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytessearct already installed\n",
      "pillow already installed\n",
      "pdf2image already installed.\n",
      "cv2 imported\n",
      "Numpy imported\n",
      "ollama already installed.\n"
     ]
    }
   ],
   "source": [
    "## tesseract: This is needed for pytesseract\n",
    "## Don't forget to install/UPDATE this in terminal : 'brew install tesseract'\n",
    "\n",
    "## Poppler: This is needed for pdf2image conversion\n",
    "## Don't forget to install/UPDATE this in terminal : 'brew install poppler'\n",
    "\n",
    "## python wrapper\n",
    "## Check if pytesseract is installed\n",
    "try:\n",
    "    import pytesseract\n",
    "    print('pytessearct already installed')\n",
    "except ImportError:\n",
    "    print('pytesseract not found. Installing...')\n",
    "    import os\n",
    "    os.system('pip install pytesseract')\n",
    "\n",
    "## Check if PIL is installed\n",
    "## Pillow to open image in a format (PIL.Image) that pytesseract understands\n",
    "try:\n",
    "    from PIL import Image\n",
    "    print('pillow already installed')\n",
    "except ImportError:\n",
    "    print('pillow not found. Instaling...')\n",
    "    import os\n",
    "    os.system('pip install pytesseract pillow')\n",
    "\n",
    "## Check if pdf2image is installed\n",
    "try:\n",
    "    from pdf2image import convert_from_path\n",
    "    print('pdf2image already installed.')\n",
    "except ImportError:\n",
    "    print('pdf2image not found. Installing...')\n",
    "    import os\n",
    "    os.system('pip install pdf2image')\n",
    "\n",
    "## Check if cv2 (OpenCV) is installed\n",
    "try:\n",
    "    import cv2\n",
    "    print(\"cv2 imported\")\n",
    "except ImportError:\n",
    "    print(\"cv2 not found. Installing...\")\n",
    "    import os\n",
    "    os.system(\"pip install opencv-python\")\n",
    "    import cv2\n",
    "\n",
    "## Check if numpy is installed\n",
    "try:\n",
    "    import numpy as np\n",
    "    print(\"Numpy imported\")\n",
    "except ImportError:\n",
    "    print(\"Numpy not imported. Installing...\")\n",
    "    import os\n",
    "    os.system(\"pip install numpy\")\n",
    "    import Numpy\n",
    "\n",
    "## Check if ollama model is installed\n",
    "try:\n",
    "    import ollama\n",
    "    print(\"ollama already installed.\")\n",
    "\n",
    "except ImportError:\n",
    "    print('ollama not found. Installing...')\n",
    "    import os\n",
    "    os.system('pip install ollama')\n",
    "\n",
    "# Import python inbuilt library\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87a0f4f5-b4cb-43bd-852f-03c103e49703",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# OpenCV FUNCTION: read png file\n",
    "\n",
    "def read_png(path, file):\n",
    "    image = os.path.join(path, file)\n",
    "    return cv2.imread(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "523e439a-779d-4f60-8bcc-0b2020bd1fa7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# display_any FUNCTION: Display ANY image within python output\n",
    "\n",
    "def display_any(image):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "\n",
    "    dpi = 80\n",
    "\n",
    "    # If image is OpenCV (numpy array)\n",
    "    if isinstance(image, np.ndarray):\n",
    "        height, width = image.shape[:2]\n",
    "        figsize = width / float(dpi), height / float(dpi)\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        ax = fig.add_axes([0, 0, 1, 1])\n",
    "        ax.axis('off')\n",
    "\n",
    "        # OpenCV loads as BGR, convert to RGB for correct display\n",
    "        if len(image.shape) == 2:  # grayscale\n",
    "            ax.imshow(image, cmap='gray')\n",
    "        else:  # color\n",
    "            ax.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # If image is PIL\n",
    "    elif isinstance(image, Image.Image):\n",
    "        width, height = image.size\n",
    "        figsize = width / float(dpi), height / float(dpi)\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        ax = fig.add_axes([0, 0, 1, 1])\n",
    "        ax.axis('off')\n",
    "\n",
    "        if image.mode == \"L\":\n",
    "            ax.imshow(image, cmap='gray')\n",
    "        else:\n",
    "            ax.imshow(image)\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2e65aba-04c5-43ad-916d-20fab83f5254",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# PIL_to_png_save FUNCTION: Save pdf file to separate .png files using pdf2image (PILLOW)\n",
    "\n",
    "def PIL_to_png_save(path_pdf):\n",
    "\n",
    "    images = convert_from_path(path_pdf) #Create PIL image object\n",
    "\n",
    "    png_folder = \"PIL_to_png\" # Name the output folder\n",
    "    os.makedirs(png_folder, exist_ok = True)  # Create the folder\n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "        image.save(os.path.join(png_folder, f\"page_{i+1}.png\"))  # Save PIL image to .png image\n",
    "\n",
    "    print(f\"{i+1} REGULAR png files from pdf saved\")\n",
    "\n",
    "\n",
    "# Calling PIL_to_png_save FUNCTION\n",
    "def call_PIL_to_png_save():\n",
    "    PIL_to_png_save('./Scanned_0002-011.pdf')\n",
    "\n",
    "# PDF PIL to png\n",
    "# call_PIL_to_png_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54b77faf-d7cb-42a7-9977-14046b055e67",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# png_to_inverted FUNCTION: Inverted Images and save to as png file\n",
    "\n",
    "def png_to_inverted(img):\n",
    "\n",
    "    if img is not None:\n",
    "        cv2_inverted_image = cv2.bitwise_not(img)\n",
    "        return cv2_inverted_image\n",
    "    \n",
    "\n",
    "\n",
    "# call_png_to_inverted FUNCTION: calls function for each file & saves it\n",
    "def call_png_to_inverted():\n",
    "\n",
    "    input_png_folder = \"PIL_to_png\"\n",
    "    output_inverted_folder = \"png_to_inverted\"\n",
    "    os.makedirs(output_inverted_folder, exist_ok = True) # Create the folder\n",
    "    \n",
    "    for chosen_png_file in os.listdir(input_png_folder):\n",
    "    \n",
    "        input_png_path = os.path.join(input_png_folder,chosen_png_file)\n",
    "        input_cv2_png_file = cv2.imread(input_png_path)\n",
    "    \n",
    "        output_inverted_path = os.path.join(output_inverted_folder, f\"INVERTED_{chosen_png_file}\")\n",
    "        \n",
    "        # call png_to_inverted function:\n",
    "        inverted_image = png_to_inverted(input_cv2_png_file)\n",
    "\n",
    "    print(\"INVERTED file created\")\n",
    "    \n",
    "#inverted Starter \n",
    "#call_png_to_inverted()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "258dc602-8f08-4335-98af-dc53a94a5c7d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# png_to_gray FUNCTION: Binarization (convert png to grayscale)\n",
    "\n",
    "def png_to_gray(input_cv2_png_file):\n",
    "\n",
    "    if input_cv2_png_file is not None:\n",
    "        cv2_gray_image = cv2.cvtColor(input_cv2_png_file, cv2.COLOR_BGR2GRAY)   # Convert png file to gray scale\n",
    "        # cv2.imshow(\"Grayscale Image\", cv2_gray_image)\n",
    "        return cv2_gray_image   # Return cv2 gray image\n",
    "\n",
    "# call png_to_gray FUNCTION: calls png to gray binarization function for each file & saves it\n",
    "\n",
    "def call_png_to_gray():\n",
    "        \n",
    "    input_png_folder = \"PIL_to_png\" # Input Folder\n",
    "    output_binary_folder = \"png_to_gray\" # Output Folder Name\n",
    "    os.makedirs(output_binary_folder, exist_ok = True) # Create Output Folder\n",
    "\n",
    "    for chosen_png_file in os.listdir(input_png_folder):\n",
    "        input_png_path = os.path.join(input_png_folder, chosen_png_file) # create input png file path\n",
    "        input_cv2_png_file = cv2.imread(input_png_path)   # Read png file\n",
    "\n",
    "        output_binary_path = os.path.join(output_binary_folder, f\"GRAY_{chosen_png_file}\")\n",
    "\n",
    "        # call png_to_gray function:\n",
    "        cv2_gray_image = png_to_gray(input_cv2_png_file)\n",
    "\n",
    "        cv2.imwrite(output_binary_path, cv2_gray_image) # Save to a file\n",
    "    print(\"GRAY file created\")\n",
    "\n",
    "\n",
    "# gray Starter\n",
    "# call_png_to_gray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "082a9f97-0edb-46cb-aaf6-2f0fdfe2a9a5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# gray_to_bw FUNCTION: cv2_gray to black and white cv2 and save as Images to png file\n",
    "\n",
    "def gray_to_bw(chosen_cv2_gray_image):\n",
    "\n",
    "   if chosen_cv2_gray_image is not None:\n",
    "        thresh, cv2_bw_image = cv2.threshold(chosen_cv2_gray_image, 210, 230, cv2.THRESH_BINARY)\n",
    "        return cv2_bw_image\n",
    "\n",
    "\n",
    "# call_gray_to_bw FUNCTION: calls function for each file & saves it\n",
    "\n",
    "def call_gray_to_bw():\n",
    "\n",
    "    input_png_folder = \"PIL_to_png\" # Input Folder\n",
    "    \n",
    "    output_bw_folder = \"gray_to_bw\" # Output Folder Name\n",
    "    os.makedirs(output_bw_folder, exist_ok = True) # Create Output Folder\n",
    "\n",
    "    for chosen_png_file in os.listdir(input_png_folder):\n",
    "        input_png_path = os.path.join(input_png_folder, chosen_png_file) # create input png file path\n",
    "        input_cv2_png_file = cv2.imread(input_png_path)   # Read png file\n",
    "\n",
    "        output_bw_path = os.path.join(output_bw_folder, f\"BW_{chosen_png_file}\")\n",
    "\n",
    "        # call png_to_gray\n",
    "        gray_image = png_to_gray(input_cv2_png_file)\n",
    "\n",
    "        #call gray_to_bw\n",
    "        cv2_bw_image= gray_to_bw(gray_image)\n",
    "\n",
    "        cv2.imwrite(output_bw_path, cv2_bw_image)\n",
    "\n",
    "    print(\"BLACK & WHITE file created\")\n",
    "\n",
    "\n",
    "# Black & White Starter\n",
    "# call_gray_to_bw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b52e0a9-070b-4902-937b-ff61ee348417",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# noise_removal FUNCTION: black and white to noise removal\n",
    "\n",
    "def noise_removal(chosen_cv2_bw_image):\n",
    "\n",
    "    if chosen_cv2_bw_image is not None:\n",
    "        kernel = np.ones((3,3), np.uint8)\n",
    "        cv2_no_noise_image = cv2.dilate(chosen_cv2_bw_image, kernel, iterations = 1)\n",
    "        cv2_no_noise_image = cv2.erode(cv2_no_noise_image, kernel, iterations =1)\n",
    "        cv2_no_noise_image = cv2.morphologyEx(cv2_no_noise_image, cv2.MORPH_CLOSE, kernel)\n",
    "        cv2_no_noise_image = cv2.medianBlur(cv2_no_noise_image, 1)\n",
    "\n",
    "        return (cv2_no_noise_image)\n",
    "\n",
    "# Call the black and white to no noise function\n",
    "def call_noise_removal():\n",
    "\n",
    "    input_png_folder = \"PIL_to_png\" # Input Folder\n",
    "    output_no_noise_folder = \"bw_to_no_noise\" # Output Folder Name\n",
    "    os.makedirs(output_no_noise_folder, exist_ok = True) # Create Output Folder\n",
    "\n",
    "    for chosen_png_file in os.listdir(input_png_folder):\n",
    "\n",
    "        input_png_path = os.path.join(input_png_folder, chosen_png_file) # create input png file path\n",
    "        input_cv2_png_file = cv2.imread(input_png_path)   # Read png file\n",
    "\n",
    "        cv2_gray_image = png_to_gray(input_cv2_png_file)  # png to gray\n",
    "        cv2_bw_image= gray_to_bw(cv2_gray_image)          # gray to bw\n",
    "        cv2_no_noise_image = noise_removal(cv2_bw_image)  # bw to no noise\n",
    "    \n",
    "        output_no_noise_path = os.path.join(output_no_noise_folder, f\"No_Noise_{chosen_png_file}\")\n",
    "        cv2.imwrite(output_no_noise_path, cv2_no_noise_image)\n",
    "\n",
    "    print(\"NO NOISE file created\")\n",
    "\n",
    "\n",
    "# noise removal Starter\n",
    "# call_noise_removal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1290c8a-b383-4275-b0bd-208d60629f8e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Erosion funtion to make font size thinner\n",
    "\n",
    "def thin_font(chosen_cv2_no_noise_image):\n",
    "\n",
    "    cv2_thin_image = cv2.bitwise_not(chosen_cv2_no_noise_image)\n",
    "    kernel = np.ones((2,2), np.uint8)\n",
    "    cv2_thin_image = cv2.erode(cv2_thin_image, kernel, iterations= 1)\n",
    "    cv2_thin_image = cv2.bitwise_not(cv2_thin_image)\n",
    "\n",
    "    return(cv2_thin_image)\n",
    "\n",
    "\n",
    "# Calling the thin_font function\n",
    "def call_thin_font():\n",
    "    input_png_folder = \"PIL_to_png\" # Input Folder\n",
    "    output_thin_folder = \"no_noise_to_thin\" # Output Folder Name\n",
    "    os.makedirs(output_thin_folder, exist_ok = True) # Create Output Folder\n",
    "    \n",
    "    for chosen_png_file in os.listdir(input_png_folder):\n",
    "\n",
    "        input_png_path = os.path.join(input_png_folder, chosen_png_file) # create input png file path\n",
    "        input_cv2_png_file = cv2.imread(input_png_path)   # Read png file\n",
    "\n",
    "        cv2_gray_image = png_to_gray(input_cv2_png_file)\n",
    "        cv2_bw_image= gray_to_bw(cv2_gray_image)\n",
    "        cv2_no_noise_image = noise_removal(cv2_bw_image)\n",
    "        cv2_thin_image = thin_font(cv2_no_noise_image)\n",
    "       \n",
    "        output_thin_path = os.path.join(output_thin_folder, f\"THIN_{chosen_png_file}\")\n",
    "        cv2.imwrite(output_thin_path, cv2_thin_image)\n",
    "\n",
    "    print(\"Thin Font file created\") \n",
    "\n",
    "\n",
    "# thin font Starter\n",
    "# call_thin_font()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "feb011a3-a459-4cfb-8355-31fc3b645903",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Dilation function to make font size thinner\n",
    "\n",
    "def thick_font(chosen_cv2_no_noise_image):\n",
    "    \n",
    "    cv2_thick_image = cv2.bitwise_not(chosen_cv2_no_noise_image)\n",
    "    kernel = np.ones((2,2), np.uint8)\n",
    "    cv2_thick_image = cv2.dilate(cv2_thick_image, kernel, iterations=1)\n",
    "    cv2_thick_image = cv2.bitwise_not(cv2_thick_image)\n",
    "\n",
    "    return(cv2_thick_image)\n",
    "\n",
    "\n",
    "# Calling the thick_font function\n",
    "def call_thick_font():    \n",
    "    input_png_folder = \"PIL_to_png\" # Input Folder\n",
    "    output_thick_folder = \"no_noise_to_thick\" # Output Folder Name\n",
    "    os.makedirs(output_thick_folder, exist_ok = True) # Create Output Folder\n",
    "    \n",
    "    for chosen_png_file in os.listdir(input_png_folder):\n",
    "\n",
    "        input_png_path = os.path.join(input_png_folder, chosen_png_file) # create input png file path\n",
    "        input_cv2_png_file = cv2.imread(input_png_path)   # Read png file\n",
    "\n",
    "        cv2_gray_image = png_to_gray(input_cv2_png_file)\n",
    "        cv2_bw_image= gray_to_bw(cv2_gray_image)\n",
    "        cv2_no_noise_image = noise_removal(cv2_bw_image)\n",
    "        cv2_thick_image = thick_font(cv2_no_noise_image)\n",
    "        output_thick_path = os.path.join(output_thick_folder, f\"THICK_{chosen_png_file}\")\n",
    "        cv2.imwrite(output_thick_path, cv2_thick_image)\n",
    "\n",
    "    print(\"Thick Font file created\") \n",
    "\n",
    "    \n",
    "\n",
    "# thick font Starter\n",
    "# call_thick_font()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e73aa1f8-f6ef-4282-9eff-ae86b598c52f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# deskew FUNCTION: Deskewing only some parts of image ( Page Segmentation Mode Applied)\n",
    "\n",
    "\n",
    "# ----- Angle per block with robust binarization -----\n",
    "def getBlockSkewAngle(blockImage) -> float:\n",
    "    gray = cv2.cvtColor(blockImage, cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    bw = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "\n",
    "    edges = cv2.Canny(bw, 50, 150, apertureSize=3)\n",
    "    lines = cv2.HoughLines(edges, 1, np.pi / 180, 100)\n",
    "    if lines is None:\n",
    "        return 0.0\n",
    "    angles = [(theta - np.pi / 2) * 180 / np.pi for rho, theta in lines[:, 0]]\n",
    "    median_angle = np.median(angles)\n",
    "    return median_angle\n",
    "\n",
    "\n",
    "def rotateImage(cvImage, angle: float):\n",
    "    (h, w) = cvImage.shape[:2]\n",
    "    M = cv2.getRotationMatrix2D((w/2, h/2), angle, 1.0)\n",
    "    return cv2.warpAffine(cvImage, M, (w, h),\n",
    "                          flags=cv2.INTER_CUBIC,\n",
    "                          borderMode=cv2.BORDER_REPLICATE)\n",
    "\n",
    "def _rotate_roi_no_clip(roi_bgr, angle_deg, pad_ratio=0.25):\n",
    "    h, w = roi_bgr.shape[:2]\n",
    "    pad = int(pad_ratio * max(h, w))\n",
    "    roi = cv2.copyMakeBorder(roi_bgr, pad, pad, pad, pad,\n",
    "                             borderType=cv2.BORDER_CONSTANT,\n",
    "                             value=(255, 255, 255))\n",
    "    (H, W) = roi.shape[:2]\n",
    "    M = cv2.getRotationMatrix2D((W/2, H/2), angle_deg, 1.0)\n",
    "    rot = cv2.warpAffine(roi, M, (W, H),\n",
    "                         flags=cv2.INTER_CUBIC,\n",
    "                         borderMode=cv2.BORDER_CONSTANT,\n",
    "                         borderValue=(255, 255, 255))\n",
    "    y0 = H//2 - h//2; y1 = y0 + h\n",
    "    x0 = W//2 - w//2; x1 = x0 + w\n",
    "    return rot[y0:y1, x0:x1]\n",
    "\n",
    "def _paste_with_mask(dst, y, x, src_rot):\n",
    "    g = cv2.cvtColor(src_rot, cv2.COLOR_BGR2GRAY)\n",
    "    _, mask = cv2.threshold(g, 250, 255, cv2.THRESH_BINARY_INV)\n",
    "    h, w = src_rot.shape[:2]\n",
    "    roi = dst[y:y+h, x:x+w]\n",
    "    m3 = cv2.merge([mask, mask, mask])\n",
    "    np.copyto(roi, src_rot, where=m3.astype(bool))\n",
    "\n",
    "def _robust_uniform(angles, tol_deg=2.0):\n",
    "    a = np.asarray(angles, float)\n",
    "    med = np.median(a)\n",
    "    mad = 1.4826 * np.median(np.abs(a - med))\n",
    "    max_dev = np.max(np.abs(a - med))\n",
    "    return med, mad, (max_dev <= tol_deg)\n",
    "\n",
    "\n",
    "def deskewHybrid(cvImage, tolerance=2.0, min_area=1500):\n",
    "    gray = cv2.cvtColor(cvImage, cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "\n",
    "    # Adaptive kernel scale\n",
    "    H, W = cvImage.shape[:2]\n",
    "    scale = max(1, int(round(max(H, W) / 1200)))\n",
    "    kernel_close = cv2.getStructuringElement(cv2.MORPH_RECT, (3*scale, 3*scale))\n",
    "    closed = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel_close)\n",
    "    dilate = cv2.dilate(closed, closed, iterations=2)\n",
    "\n",
    "\n",
    "    contours, _ = cv2.findContours(dilate, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Filter contours; avoid lines/logos by aspect/area guards (optional)\n",
    "    margin_x = int(0.05 * W)\n",
    "    margin_y = int(0.05 * H)\n",
    "\n",
    "    # Get bounding boxes for all contours\n",
    "    boxes = [cv2.boundingRect(c) for c in contours]\n",
    "    \n",
    "    filtered = [\n",
    "    (x, y, w, h)\n",
    "    for (x, y, w, h) in boxes\n",
    "    if x > margin_x and x + w < W - margin_x\n",
    "    and y > margin_y and y + h < H - margin_y\n",
    "    ]\n",
    "    for c in contours:\n",
    "        x, y, w, h = cv2.boundingRect(c)\n",
    "        if w*h < min_area:\n",
    "            continue\n",
    "        ar = w / max(1.0, h)\n",
    "        if ar < 1.5 and h < 20:  # likely a small vertical tick/line\n",
    "            continue\n",
    "        filtered.append((x, y, w, h))\n",
    "\n",
    "    if not filtered:\n",
    "        return cvImage\n",
    "\n",
    "    block_angles = []\n",
    "    for (x, y, w, h) in filtered:\n",
    "        block = cvImage[y:y+h, x:x+w]\n",
    "        ang = getBlockSkewAngle(block)\n",
    "        block_angles.append(ang)\n",
    "\n",
    "    med, mad, is_uniform = _robust_uniform(block_angles, tol_deg=tolerance)\n",
    "\n",
    "    if is_uniform:\n",
    "        # Page-level correction using robust median\n",
    "        return rotateImage(cvImage, med)\n",
    "\n",
    "    # Block-level correction with padding + masked paste\n",
    "    out = cvImage.copy()\n",
    "    for (x, y, w, h), ang in zip(filtered, block_angles):\n",
    "        if abs(ang) < 0.6:  # skip almost-straight blocks\n",
    "            continue\n",
    "        roi = cvImage[y:y+h, x:x+w]\n",
    "        rot = _rotate_roi_no_clip(roi, ang, pad_ratio=0.35)\n",
    "        _paste_with_mask(out, y, x, rot)\n",
    "    return out\n",
    "\n",
    "\n",
    "# This deskew algorithm didn't work properly. Check the dewarp() algorithm down the line\n",
    "# Call deskewHybrid function\n",
    "def call_deskewHybrid():\n",
    "\n",
    "    input_png_folder = \"PIL_to_png\" # Input Folder\n",
    "    output_deskewed_folder = \"deskewed\"\n",
    "    os.makedirs(output_deskewed_folder, exist_ok = True) # Create the folder\n",
    "\n",
    "    for chosen_png_file in os.listdir(input_png_folder):\n",
    "        input_png_path = os.path.join(input_png_folder, chosen_png_file) # create input png file path\n",
    "        input_cv2_png_file = cv2.imread(input_png_path)   # Read png file\n",
    "        \n",
    "        cv2_image_deskewed = deskewHybrid(input_cv2_png_file)\n",
    "        output_deskewed_path = os.path.join(output_deskewed_folder, f\"DESKEWED_{chosen_png_file}\")\n",
    "        cv2.imwrite(output_deskewed_path, cv2_image_deskewed)\n",
    "\n",
    "    print(\"Deskewed file created\")\n",
    "    \n",
    "\n",
    "# Deskew Starter\n",
    "# call_deskewHybrid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cf4f357d-cd44-4416-ba0b-540fa6b3eabc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Dewarp function to remap the image\n",
    "import os\n",
    "import datetime\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# for some reason pylint complains about cv2 members being undefined :(\n",
    "# pylint: disable=E1101\n",
    "\n",
    "PAGE_MARGIN_X = 50       # reduced px to ignore near L/R edge\n",
    "PAGE_MARGIN_Y = 20       # reduced px to ignore near T/B edge\n",
    "\n",
    "OUTPUT_ZOOM = 1.0        # how much to zoom output relative to *original* image\n",
    "OUTPUT_DPI = 300         # just affects stated DPI of PNG, not appearance\n",
    "REMAP_DECIMATE = 16      # downscaling factor for remapping image\n",
    "\n",
    "ADAPTIVE_WINSZ = 55      # window size for adaptive threshold in reduced px\n",
    "\n",
    "TEXT_MIN_WIDTH = 15      # min reduced px width of detected text contour\n",
    "TEXT_MIN_HEIGHT = 2      # min reduced px height of detected text contour\n",
    "TEXT_MIN_ASPECT = 1.5    # filter out text contours below this w/h ratio\n",
    "TEXT_MAX_THICKNESS = 10  # max reduced px thickness of detected text contour\n",
    "\n",
    "EDGE_MAX_OVERLAP = 1.0   # max reduced px horiz. overlap of contours in span\n",
    "EDGE_MAX_LENGTH = 100.0  # max reduced px length of edge connecting contours\n",
    "EDGE_ANGLE_COST = 10.0   # cost of angles in edges (tradeoff vs. length)\n",
    "EDGE_MAX_ANGLE = 7.5     # maximum change in angle allowed between contours\n",
    "\n",
    "RVEC_IDX = slice(0, 3)   # index of rvec in params vector\n",
    "TVEC_IDX = slice(3, 6)   # index of tvec in params vector\n",
    "CUBIC_IDX = slice(6, 8)  # index of cubic slopes in params vector\n",
    "\n",
    "SPAN_MIN_WIDTH = 30      # minimum reduced px width for span\n",
    "SPAN_PX_PER_STEP = 20    # reduced px spacing for sampling along spans\n",
    "FOCAL_LENGTH = 1.2       # normalized focal length of camera\n",
    "\n",
    "# Set DEBUG_LEVEL to 1 to save intermediate processing steps as images\n",
    "DEBUG_LEVEL = 0          # 0=none, 1=some, 2=lots, 3=all\n",
    "DEBUG_OUTPUT = 'file'    # file, screen, both\n",
    "\n",
    "# nice color palette for visualizing contours, etc.\n",
    "CCOLORS = [\n",
    "    (255, 0, 0), (255, 63, 0), (255, 127, 0), (255, 191, 0),\n",
    "    (255, 255, 0), (191, 255, 0), (127, 255, 0), (63, 255, 0),\n",
    "    (0, 255, 0), (0, 255, 63), (0, 255, 127), (0, 255, 191),\n",
    "    (0, 255, 255), (0, 191, 255), (0, 127, 255), (0, 63, 255),\n",
    "    (0, 0, 255), (63, 0, 255), (127, 0, 255), (191, 0, 255),\n",
    "    (255, 0, 255), (255, 0, 191), (255, 0, 127), (255, 0, 63)\n",
    "]\n",
    "\n",
    "# default intrinsic parameter matrix\n",
    "K = np.array([\n",
    "    [FOCAL_LENGTH, 0, 0],\n",
    "    [0, FOCAL_LENGTH, 0],\n",
    "    [0, 0, 1]], dtype=np.float32)\n",
    "\n",
    "\n",
    "\n",
    "def debug_show(name, step, text, display):\n",
    "    # cv2.imshow is not ideal for notebooks. This function will save debug images if DEBUG_LEVEL > 0.\n",
    "    if DEBUG_OUTPUT != 'screen':\n",
    "        filetext = text.replace(' ', '_')\n",
    "        outfile = name + '_debug_' + str(step) + '_' + filetext + '.png'\n",
    "        #cv2.imwrite(outfile, display)\n",
    "\n",
    "def round_nearest_multiple(i, factor):\n",
    "    i = int(i)\n",
    "    rem = i % factor\n",
    "    if not rem:\n",
    "        return i\n",
    "    else:\n",
    "        return i + factor - rem\n",
    "\n",
    "def pix2norm(shape, pts):\n",
    "    height, width = shape[:2]\n",
    "    scl = 2.0/(max(height, width))\n",
    "    offset = np.array([width, height], dtype=pts.dtype).reshape((-1, 1, 2))*0.5\n",
    "    return (pts - offset) * scl\n",
    "\n",
    "def norm2pix(shape, pts, as_integer):\n",
    "    height, width = shape[:2]\n",
    "    scl = max(height, width)*0.5\n",
    "    offset = np.array([0.5*width, 0.5*height],\n",
    "                      dtype=pts.dtype).reshape((-1, 1, 2))\n",
    "    rval = pts * scl + offset\n",
    "    if as_integer:\n",
    "        return (rval + 0.5).astype(int)\n",
    "    else:\n",
    "        return rval\n",
    "\n",
    "def fltp(point):\n",
    "    return tuple(point.astype(int).flatten())\n",
    "\n",
    "def draw_correspondences(img, dstpoints, projpts):\n",
    "    display = img.copy()\n",
    "    dstpoints = norm2pix(img.shape, dstpoints, True)\n",
    "    projpts = norm2pix(img.shape, projpts, True)\n",
    "    for pts, color in [(projpts, (255, 0, 0)),\n",
    "                       (dstpoints, (0, 0, 255))]:\n",
    "        for point in pts:\n",
    "            cv2.circle(display, fltp(point), 3, color, -1, cv2.LINE_AA)\n",
    "    for point_a, point_b in zip(projpts, dstpoints):\n",
    "        cv2.line(display, fltp(point_a), fltp(point_b),\n",
    "                 (255, 255, 255), 1, cv2.LINE_AA)\n",
    "    return display\n",
    "\n",
    "def get_default_params(corners, ycoords, xcoords):\n",
    "    page_width = np.linalg.norm(corners[1] - corners[0])\n",
    "    page_height = np.linalg.norm(corners[-1] - corners[0])\n",
    "    rough_dims = (page_width, page_height)\n",
    "    cubic_slopes = [0.0, 0.0]\n",
    "    corners_object3d = np.array([\n",
    "        [0, 0, 0],\n",
    "        [page_width, 0, 0],\n",
    "        [page_width, page_height, 0],\n",
    "        [0, page_height, 0]])\n",
    "    _, rvec, tvec = cv2.solvePnP(corners_object3d,\n",
    "                                 corners, K, np.zeros(5))\n",
    "    span_counts = [len(xc) for xc in xcoords]\n",
    "    params = np.hstack((np.array(rvec).flatten(),\n",
    "                        np.array(tvec).flatten(),\n",
    "                        np.array(cubic_slopes).flatten(),\n",
    "                        ycoords.flatten()) +\n",
    "                       tuple(xcoords))\n",
    "    return rough_dims, span_counts, params\n",
    "\n",
    "def project_xy(xy_coords, pvec):\n",
    "    alpha, beta = tuple(pvec[CUBIC_IDX])\n",
    "    poly = np.array([\n",
    "        alpha + beta,\n",
    "        -2*alpha - beta,\n",
    "        alpha,\n",
    "        0])\n",
    "    xy_coords = xy_coords.reshape((-1, 2))\n",
    "    z_coords = np.polyval(poly, xy_coords[:, 0])\n",
    "    objpoints = np.hstack((xy_coords, z_coords.reshape((-1, 1))))\n",
    "    image_points, _ = cv2.projectPoints(objpoints,\n",
    "                                        pvec[RVEC_IDX],\n",
    "                                        pvec[TVEC_IDX],\n",
    "                                        K, np.zeros(5))\n",
    "    return image_points\n",
    "\n",
    "def project_keypoints(pvec, keypoint_index):\n",
    "    xy_coords = pvec[keypoint_index]\n",
    "    xy_coords[0, :] = 0\n",
    "    return project_xy(xy_coords, pvec)\n",
    "\n",
    "def resize_to_screen(src, maxw=1280, maxh=700, copy=False):\n",
    "    height, width = src.shape[:2]\n",
    "    scl_x = float(width)/maxw\n",
    "    scl_y = float(height)/maxh\n",
    "    scl = int(np.ceil(max(scl_x, scl_y)))\n",
    "    if scl > 1.0:\n",
    "        inv_scl = 1.0/scl\n",
    "        img = cv2.resize(src, (0, 0), None, inv_scl, inv_scl, cv2.INTER_AREA)\n",
    "    elif copy:\n",
    "        img = src.copy()\n",
    "    else:\n",
    "        img = src\n",
    "    return img\n",
    "\n",
    "def box(width, height):\n",
    "    return np.ones((height, width), dtype=np.uint8)\n",
    "\n",
    "def get_page_extents(small):\n",
    "    height, width = small.shape[:2]\n",
    "    xmin, ymin = PAGE_MARGIN_X, PAGE_MARGIN_Y\n",
    "    xmax, ymax = width - PAGE_MARGIN_X, height - PAGE_MARGIN_Y\n",
    "    page = np.zeros((height, width), dtype=np.uint8)\n",
    "    cv2.rectangle(page, (xmin, ymin), (xmax, ymax), (255, 255, 255), -1)\n",
    "    outline = np.array([[xmin, ymin], [xmin, ymax], [xmax, ymax], [xmax, ymin]])\n",
    "    return page, outline    \n",
    "    \n",
    "    \"\"\"\n",
    "    SMART FIX: Automatically detect content boundaries.\n",
    "    Uses actual content to determine where the page edges are.\n",
    "    \"\"\"\n",
    "    height, width = small.shape[:2]\n",
    "    \n",
    "    # Convert to grayscale if needed\n",
    "    if len(small.shape) == 3:\n",
    "        gray = cv2.cvtColor(small, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray = small\n",
    "    \n",
    "    # Threshold to find content\n",
    "    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    \n",
    "    # Find content boundaries\n",
    "    coords = cv2.findNonZero(binary)\n",
    "    \n",
    "    if coords is not None:\n",
    "        # Get bounding box of actual content\n",
    "        x, y, w, h = cv2.boundingRect(coords)\n",
    "        \n",
    "        # Add small buffer around content (2% of image size)\n",
    "        buffer_x = int(width * 0.02)\n",
    "        buffer_y = int(height * 0.02)\n",
    "        \n",
    "        xmin = max(0, x - buffer_x)\n",
    "        ymin = max(0, y - buffer_y)\n",
    "        xmax = min(width, x + w + buffer_x)\n",
    "        ymax = min(height, y + h + buffer_y)\n",
    "    else:\n",
    "        # Fallback: use small margins\n",
    "        margin = 5\n",
    "        xmin, ymin = margin, margin\n",
    "        xmax, ymax = width - margin, height - margin\n",
    "    \n",
    "    page = np.zeros((height, width), dtype=np.uint8)\n",
    "    cv2.rectangle(page, (xmin, ymin), (xmax, ymax), (255, 255, 255), -1)\n",
    "    outline = np.array([[xmin, ymin], [xmin, ymax], [xmax, ymax], [xmax, ymin]])\n",
    "    \n",
    "    print(f\"  ðŸŽ¯ Auto-detected page bounds: {xmin},{ymin} to {xmax},{ymax}\")\n",
    "    \n",
    "    return page, outline\n",
    "\n",
    "def get_mask(name, small, pagemask, masktype):\n",
    "    sgray = cv2.cvtColor(small, cv2.COLOR_RGB2GRAY)\n",
    "    if masktype == 'text':\n",
    "        mask = cv2.adaptiveThreshold(sgray, 255, cv2.ADAPTIVE_THRESH_MEAN_C,\n",
    "                                     cv2.THRESH_BINARY_INV, ADAPTIVE_WINSZ, 25)\n",
    "        if DEBUG_LEVEL >= 3: debug_show(name, 0.1, 'thresholded', mask)\n",
    "        mask = cv2.dilate(mask, box(9, 1))\n",
    "        if DEBUG_LEVEL >= 3: debug_show(name, 0.2, 'dilated', mask)\n",
    "        mask = cv2.erode(mask, box(1, 3))\n",
    "        if DEBUG_LEVEL >= 3: debug_show(name, 0.3, 'eroded', mask)\n",
    "    else:\n",
    "        mask = cv2.adaptiveThreshold(sgray, 255, cv2.ADAPTIVE_THRESH_MEAN_C,\n",
    "                                     cv2.THRESH_BINARY_INV, ADAPTIVE_WINSZ, 7)\n",
    "        if DEBUG_LEVEL >= 3: debug_show(name, 0.4, 'thresholded', mask)\n",
    "        mask = cv2.erode(mask, box(3, 1), iterations=3)\n",
    "        if DEBUG_LEVEL >= 3: debug_show(name, 0.5, 'eroded', mask)\n",
    "        mask = cv2.dilate(mask, box(8, 2))\n",
    "        if DEBUG_LEVEL >= 3: debug_show(name, 0.6, 'dilated', mask)\n",
    "    return np.minimum(mask, pagemask)\n",
    "\n",
    "def interval_measure_overlap(int_a, int_b):\n",
    "    return min(int_a[1], int_b[1]) - max(int_a[0], int_b[0])\n",
    "\n",
    "def angle_dist(angle_b, angle_a):\n",
    "    diff = angle_b - angle_a\n",
    "    while diff > np.pi: diff -= 2*np.pi\n",
    "    while diff < -np.pi: diff += 2*np.pi\n",
    "    return np.abs(diff)\n",
    "\n",
    "def blob_mean_and_tangent(contour):\n",
    "    moments = cv2.moments(contour)\n",
    "    area = moments['m00']\n",
    "    mean_x = moments['m10'] / area\n",
    "    mean_y = moments['m01'] / area\n",
    "    moments_matrix = np.array([[moments['mu20'], moments['mu11']],\n",
    "                               [moments['mu11'], moments['mu02']]]) / area\n",
    "    _, svd_u, _ = cv2.SVDecomp(moments_matrix)\n",
    "    center = np.array([mean_x, mean_y])\n",
    "    tangent = svd_u[:, 0].flatten().copy()\n",
    "    return center, tangent\n",
    "\n",
    "class ContourInfo(object):\n",
    "    def __init__(self, contour, rect, mask):\n",
    "        self.contour = contour\n",
    "        self.rect = rect\n",
    "        self.mask = mask\n",
    "        self.center, self.tangent = blob_mean_and_tangent(contour)\n",
    "        self.angle = np.arctan2(self.tangent[1], self.tangent[0])\n",
    "        clx = [self.proj_x(point) for point in contour]\n",
    "        lxmin, lxmax = min(clx), max(clx)\n",
    "        self.local_xrng = (lxmin, lxmax)\n",
    "        self.point0 = self.center + self.tangent * lxmin\n",
    "        self.point1 = self.center + self.tangent * lxmax\n",
    "        self.pred = self.succ = None\n",
    "    def proj_x(self, point):\n",
    "        return np.dot(self.tangent, point.flatten()-self.center)\n",
    "    def local_overlap(self, other):\n",
    "        xmin = self.proj_x(other.point0)\n",
    "        xmax = self.proj_x(other.point1)\n",
    "        return interval_measure_overlap(self.local_xrng, (xmin, xmax))\n",
    "\n",
    "def generate_candidate_edge(cinfo_a, cinfo_b):\n",
    "    if cinfo_a.point0[0] > cinfo_b.point1[0]:\n",
    "        cinfo_a, cinfo_b = cinfo_b, cinfo_a\n",
    "    x_overlap_a = cinfo_a.local_overlap(cinfo_b)\n",
    "    x_overlap_b = cinfo_b.local_overlap(cinfo_a)\n",
    "    overall_tangent = cinfo_b.center - cinfo_a.center\n",
    "    overall_angle = np.arctan2(overall_tangent[1], overall_tangent[0])\n",
    "    delta_angle = max(angle_dist(cinfo_a.angle, overall_angle),\n",
    "                      angle_dist(cinfo_b.angle, overall_angle)) * 180/np.pi\n",
    "    x_overlap = max(x_overlap_a, x_overlap_b)\n",
    "    dist = np.linalg.norm(cinfo_b.point0 - cinfo_a.point1)\n",
    "    if (dist > EDGE_MAX_LENGTH or x_overlap > EDGE_MAX_OVERLAP or delta_angle > EDGE_MAX_ANGLE):\n",
    "        return None\n",
    "    score = dist + delta_angle*EDGE_ANGLE_COST\n",
    "    return (score, cinfo_a, cinfo_b)\n",
    "\n",
    "def make_tight_mask(contour, xmin, ymin, width, height):\n",
    "    tight_mask = np.zeros((height, width), dtype=np.uint8)\n",
    "    tight_contour = contour - np.array((xmin, ymin)).reshape((-1, 1, 2))\n",
    "    cv2.drawContours(tight_mask, [tight_contour], 0, (1, 1, 1), -1)\n",
    "    return tight_mask\n",
    "\n",
    "def get_contours(name, small, pagemask, masktype):\n",
    "    mask = get_mask(name, small, pagemask, masktype)\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)[-2:]\n",
    "    contours_out = []\n",
    "    for contour in contours:\n",
    "        rect = cv2.boundingRect(contour)\n",
    "        xmin, ymin, width, height = rect\n",
    "        if (width < TEXT_MIN_WIDTH or height < TEXT_MIN_HEIGHT or width < TEXT_MIN_ASPECT*height):\n",
    "            continue\n",
    "        tight_mask = make_tight_mask(contour, xmin, ymin, width, height)\n",
    "        if tight_mask.sum(axis=0).max() > TEXT_MAX_THICKNESS:\n",
    "            continue\n",
    "        contours_out.append(ContourInfo(contour, rect, tight_mask))\n",
    "    if DEBUG_LEVEL >= 2: visualize_contours(name, small, contours_out)\n",
    "    return contours_out\n",
    "\n",
    "def assemble_spans(name, small, pagemask, cinfo_list):\n",
    "    cinfo_list = sorted(cinfo_list, key=lambda cinfo: cinfo.rect[1])\n",
    "    candidate_edges = []\n",
    "    for i, cinfo_i in enumerate(cinfo_list):\n",
    "        for j in range(i):\n",
    "            edge = generate_candidate_edge(cinfo_i, cinfo_list[j])\n",
    "            if edge: candidate_edges.append(edge)\n",
    "    candidate_edges.sort()\n",
    "    for _, cinfo_a, cinfo_b in candidate_edges:\n",
    "        if cinfo_a.succ is None and cinfo_b.pred is None:\n",
    "            cinfo_a.succ, cinfo_b.pred = cinfo_b, cinfo_a\n",
    "    spans = []\n",
    "    while cinfo_list:\n",
    "        cinfo = cinfo_list[0]\n",
    "        while cinfo.pred: cinfo = cinfo.pred\n",
    "        cur_span, width = [], 0.0\n",
    "        while cinfo:\n",
    "            cinfo_list.remove(cinfo)\n",
    "            cur_span.append(cinfo)\n",
    "            width += cinfo.local_xrng[1] - cinfo.local_xrng[0]\n",
    "            cinfo = cinfo.succ\n",
    "        if width > SPAN_MIN_WIDTH:\n",
    "            spans.append(cur_span)\n",
    "    if DEBUG_LEVEL >= 2: visualize_spans(name, small, pagemask, spans)\n",
    "    return spans\n",
    "\n",
    "def sample_spans(shape, spans):\n",
    "    span_points = []\n",
    "    for span in spans:\n",
    "        contour_points = []\n",
    "        for cinfo in span:\n",
    "            yvals = np.arange(cinfo.mask.shape[0]).reshape((-1, 1))\n",
    "            totals = (yvals * cinfo.mask).sum(axis=0)\n",
    "            means = totals / cinfo.mask.sum(axis=0)\n",
    "            xmin, ymin = cinfo.rect[:2]\n",
    "            step = SPAN_PX_PER_STEP\n",
    "            start = ((len(means)-1) % step) // 2\n",
    "            contour_points += [(x+xmin, means[x]+ymin) for x in range(start, len(means), step)]\n",
    "        contour_points = np.array(contour_points, dtype=np.float32).reshape((-1, 1, 2))\n",
    "        contour_points = pix2norm(shape, contour_points)\n",
    "        span_points.append(contour_points)\n",
    "    return span_points\n",
    "\n",
    "def keypoints_from_samples(name, small, pagemask, page_outline, span_points):\n",
    "    all_evecs, all_weights = np.array([[0.0, 0.0]]), 0\n",
    "    for points in span_points:\n",
    "        _, evec = cv2.PCACompute(points.reshape((-1, 2)), None, maxComponents=1)\n",
    "        weight = np.linalg.norm(points[-1] - points[0])\n",
    "        all_evecs += evec * weight\n",
    "        all_weights += weight\n",
    "    evec = all_evecs / all_weights\n",
    "    x_dir = evec.flatten()\n",
    "    if x_dir[0] < 0: x_dir = -x_dir\n",
    "    y_dir = np.array([-x_dir[1], x_dir[0]])\n",
    "    pagecoords = cv2.convexHull(page_outline)\n",
    "    pagecoords = pix2norm(pagemask.shape, pagecoords.reshape((-1, 1, 2))).reshape((-1, 2))\n",
    "    px_coords = np.dot(pagecoords, x_dir)\n",
    "    py_coords = np.dot(pagecoords, y_dir)\n",
    "    px0, px1 = px_coords.min(), px_coords.max()\n",
    "    py0, py1 = py_coords.min(), py_coords.max()\n",
    "    p00 = px0 * x_dir + py0 * y_dir\n",
    "    p10 = px1 * x_dir + py0 * y_dir\n",
    "    p11 = px1 * x_dir + py1 * y_dir\n",
    "    p01 = px0 * x_dir + py1 * y_dir\n",
    "    corners = np.vstack((p00, p10, p11, p01)).reshape((-1, 1, 2))\n",
    "    ycoords, xcoords = [], []\n",
    "    for points in span_points:\n",
    "        pts = points.reshape((-1, 2))\n",
    "        px_coords = np.dot(pts, x_dir)\n",
    "        py_coords = np.dot(pts, y_dir)\n",
    "        ycoords.append(py_coords.mean() - py0)\n",
    "        xcoords.append(px_coords - px0)\n",
    "    if DEBUG_LEVEL >= 2: visualize_span_points(name, small, span_points, corners)\n",
    "    return corners, np.array(ycoords), xcoords\n",
    "\n",
    "def visualize_contours(name, small, cinfo_list):\n",
    "    # This function is for debugging and will save images to disk.\n",
    "    regions = np.zeros_like(small)\n",
    "    for j, cinfo in enumerate(cinfo_list):\n",
    "        cv2.drawContours(regions, [cinfo.contour], 0, CCOLORS[j % len(CCOLORS)], -1)\n",
    "    mask = (regions.max(axis=2) != 0)\n",
    "    display = small.copy()\n",
    "    display[mask] = (display[mask]/2) + (regions[mask]/2)\n",
    "    for j, cinfo in enumerate(cinfo_list):\n",
    "        cv2.circle(display, fltp(cinfo.center), 3, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "        cv2.line(display, fltp(cinfo.point0), fltp(cinfo.point1), (255, 255, 255), 1, cv2.LINE_AA)\n",
    "    debug_show(name, 1, 'contours', display)\n",
    "\n",
    "def visualize_spans(name, small, pagemask, spans):\n",
    "    # This function is for debugging and will save images to disk.\n",
    "    regions = np.zeros_like(small)\n",
    "    for i, span in enumerate(spans):\n",
    "        contours = [cinfo.contour for cinfo in span]\n",
    "        cv2.drawContours(regions, contours, -1, CCOLORS[i*3 % len(CCOLORS)], -1)\n",
    "    mask = (regions.max(axis=2) != 0)\n",
    "    display = small.copy()\n",
    "    display[mask] = (display[mask]/2) + (regions[mask]/2)\n",
    "    display[pagemask == 0] //= 4\n",
    "    debug_show(name, 2, 'spans', display)\n",
    "\n",
    "def visualize_span_points(name, small, span_points, corners):\n",
    "    # This function is for debugging and will save images to disk.\n",
    "    display = small.copy()\n",
    "    for i, points in enumerate(span_points):\n",
    "        points = norm2pix(small.shape, points, False)\n",
    "        mean, small_evec = cv2.PCACompute(points.reshape((-1, 2)), None, maxComponents=1)\n",
    "        dps = np.dot(points.reshape((-1, 2)), small_evec.reshape((2, 1)))\n",
    "        dpm = np.dot(mean.flatten(), small_evec.flatten())\n",
    "        point0 = mean + small_evec * (dps.min()-dpm)\n",
    "        point1 = mean + small_evec * (dps.max()-dpm)\n",
    "        for point in points:\n",
    "            cv2.circle(display, fltp(point), 3, CCOLORS[i % len(CCOLORS)], -1, cv2.LINE_AA)\n",
    "        cv2.line(display, fltp(point0), fltp(point1), (255, 255, 255), 1, cv2.LINE_AA)\n",
    "    cv2.polylines(display, [norm2pix(small.shape, corners, True)], True, (255, 255, 255))\n",
    "    debug_show(name, 3, 'span points', display)\n",
    "\n",
    "def imgsize(img):\n",
    "    height, width = img.shape[:2]\n",
    "    return '{}x{}'.format(width, height)\n",
    "\n",
    "def make_keypoint_index(span_counts):\n",
    "    nspans = len(span_counts)\n",
    "    npts = sum(span_counts)\n",
    "    keypoint_index = np.zeros((npts+1, 2), dtype=int)\n",
    "    start = 1\n",
    "    for i, count in enumerate(span_counts):\n",
    "        end = start + count\n",
    "        keypoint_index[start:start+end, 1] = 8+i\n",
    "        start = end\n",
    "    keypoint_index[1:, 0] = np.arange(npts) + 8 + nspans\n",
    "    return keypoint_index\n",
    "\n",
    "def optimize_params(name, small, dstpoints, span_counts, params):\n",
    "    keypoint_index = make_keypoint_index(span_counts)\n",
    "    def objective(pvec):\n",
    "        ppts = project_keypoints(pvec, keypoint_index)\n",
    "        return np.sum((dstpoints - ppts)**2)\n",
    "    print(f'Initial objective is {objective(params)}')\n",
    "    if DEBUG_LEVEL >= 1:\n",
    "        projpts = project_keypoints(params, keypoint_index)\n",
    "        display = draw_correspondences(small, dstpoints, projpts)\n",
    "        debug_show(name, 4, 'keypoints_before', display)\n",
    "    print(f'Optimizing {len(params)} parameters...')\n",
    "    start = datetime.datetime.now()\n",
    "    res = scipy.optimize.minimize(objective, params, method='Powell')\n",
    "    end = datetime.datetime.now()\n",
    "    print(f'Optimization took {(end-start).total_seconds():.2f} sec.')\n",
    "    print(f'Final objective is {res.fun}')\n",
    "    params = res.x\n",
    "    if DEBUG_LEVEL >= 1:\n",
    "        projpts = project_keypoints(params, keypoint_index)\n",
    "        display = draw_correspondences(small, dstpoints, projpts)\n",
    "        debug_show(name, 5, 'keypoints_after', display)\n",
    "    return params\n",
    "\n",
    "def get_page_dims(corners, rough_dims, params):\n",
    "    dst_br = corners[2].flatten()\n",
    "    dims = np.array(rough_dims)\n",
    "    def objective(dims):\n",
    "        proj_br = project_xy(dims, params)\n",
    "        return np.sum((dst_br - proj_br.flatten())**2)\n",
    "    res = scipy.optimize.minimize(objective, dims, method='Powell')\n",
    "    dims = res.x\n",
    "    print(f'Got page dims {dims[0]} x {dims[1]}')\n",
    "    return dims\n",
    "\n",
    "def remap_image(name, img, small, page_dims, params):\n",
    "    height = 0.5 * page_dims[1] * OUTPUT_ZOOM * img.shape[0]\n",
    "    height = round_nearest_multiple(height, REMAP_DECIMATE)\n",
    "    width = round_nearest_multiple(height * page_dims[0] / page_dims[1], REMAP_DECIMATE)\n",
    "    print(f'Output will be {width}x{height}')\n",
    "    height_small = height // REMAP_DECIMATE\n",
    "    width_small = width // REMAP_DECIMATE\n",
    "    page_x_range = np.linspace(0, page_dims[0], width_small)\n",
    "    page_y_range = np.linspace(0, page_dims[1], height_small)\n",
    "    page_x_coords, page_y_coords = np.meshgrid(page_x_range, page_y_range)\n",
    "    page_xy_coords = np.hstack((page_x_coords.flatten().reshape((-1, 1)),\n",
    "                                page_y_coords.flatten().reshape((-1, 1))))\n",
    "    page_xy_coords = page_xy_coords.astype(np.float32)\n",
    "    image_points = project_xy(page_xy_coords, params)\n",
    "    image_points = norm2pix(img.shape, image_points, False)\n",
    "    image_x_coords = image_points[:, 0, 0].reshape(page_x_coords.shape)\n",
    "    image_y_coords = image_points[:, 0, 1].reshape(page_y_coords.shape)\n",
    "    image_x_coords = cv2.resize(image_x_coords, (width, height), interpolation=cv2.INTER_CUBIC)\n",
    "    image_y_coords = cv2.resize(image_y_coords, (width, height), interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "    # Use BGR image for remap if it's color, otherwise use the image as is\n",
    "    if len(img.shape) == 3 and img.shape[2] == 3:\n",
    "        remapped_color = cv2.remap(img, image_x_coords, image_y_coords,\n",
    "                                   cv2.INTER_CUBIC, None, cv2.BORDER_REPLICATE)\n",
    "        remapped_gray = cv2.cvtColor(remapped_color, cv2.COLOR_BGR2GRAY)\n",
    "    else: # Already grayscale\n",
    "        remapped_gray = cv2.remap(img, image_x_coords, image_y_coords,\n",
    "                                  cv2.INTER_CUBIC, None, cv2.BORDER_REPLICATE)\n",
    "\n",
    "    thresh = cv2.adaptiveThreshold(remapped_gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C,\n",
    "                                   cv2.THRESH_BINARY, ADAPTIVE_WINSZ, 25)\n",
    "    pil_image = Image.fromarray(thresh)\n",
    "    pil_image = pil_image.convert('1')\n",
    "    threshfile = name + '_thresh.png'\n",
    "    #pil_image.save(threshfile, dpi=(OUTPUT_DPI, OUTPUT_DPI))\n",
    "    if DEBUG_LEVEL >= 1:\n",
    "        height = small.shape[0]\n",
    "        width = int(round(height * float(thresh.shape[1])/thresh.shape[0]))\n",
    "        display = cv2.resize(thresh, (width, height), interpolation=cv2.INTER_AREA)\n",
    "        debug_show(name, 6, 'output', display)\n",
    "    return threshfile, remapped_gray, thresh\n",
    "\n",
    "# file_access_helper FUNCTION: RUN THIS CELL TO PROCESS THE IMAGE\n",
    "\n",
    "def file_assess_helper(image_path):\n",
    "\n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    try:\n",
    "        if img is None:\n",
    "            raise FileNotFoundError(f\"Could not load image at path: {image_path}\")\n",
    "            \n",
    "        small = resize_to_screen(img)\n",
    "        basename = os.path.basename(image_path)\n",
    "        name, _ = os.path.splitext(basename)\n",
    "    \n",
    "        #print(f\"Loaded '{basename}' with size {imgsize(img)}, resized to {imgsize(small)}\")\n",
    "    \n",
    "        if DEBUG_LEVEL >= 3:\n",
    "            debug_show(name, 0.0, 'original', small)\n",
    "    \n",
    "        pagemask, page_outline = get_page_extents(small)\n",
    "    \n",
    "        cinfo_list = get_contours(name, small, pagemask, 'text')\n",
    "        spans = assemble_spans(name, small, pagemask, cinfo_list)\n",
    "    \n",
    "        if len(spans) < 3:\n",
    "            print(f\"Detecting lines because only {len(spans)} text spans were found.\")\n",
    "            cinfo_list = get_contours(name, small, pagemask, 'line')\n",
    "            spans2 = assemble_spans(name, small, pagemask, cinfo_list)\n",
    "            if len(spans2) > len(spans):\n",
    "                spans = spans2\n",
    "    \n",
    "        if len(spans) < 1:\n",
    "            print(f\"Skipping '{name}' because only {len(spans)} spans were found.\")\n",
    "        else:\n",
    "            span_points = sample_spans(small.shape, spans)\n",
    "            print(f\"Got {len(spans)} spans with {sum([len(pts) for pts in span_points])} points.\")\n",
    "    \n",
    "            corners, ycoords, xcoords = keypoints_from_samples(name, small, pagemask, page_outline, span_points)\n",
    "            rough_dims, span_counts, params = get_default_params(corners, ycoords, xcoords)\n",
    "    \n",
    "            dstpoints = np.vstack((corners[0].reshape((1, 1, 2)),) + tuple(span_points))\n",
    "    \n",
    "            params = optimize_params(name, small, dstpoints, span_counts, params)\n",
    "            page_dims = get_page_dims(corners, rough_dims, params)\n",
    "            outfile, remapped_img, cv2_thresh_img_dewarped = remap_image(name, img, small, page_dims, params)\n",
    "\n",
    "            return cv2_thresh_img_dewarped\n",
    "                                               \n",
    "            #print(f\"Wrote dewarped and thresholded image to: '{outfile}'\")\n",
    "    \n",
    "            # Display the results in the notebook\n",
    "            # plt.figure(figsize=(15, 5))\n",
    "            # plt.subplot(1, 3, 1)\n",
    "            # plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "            # plt.title('Original Image')\n",
    "            # plt.axis('off')\n",
    "            \n",
    "            # plt.subplot(1, 3, 2)\n",
    "            # plt.imshow(remapped_img, cmap='gray')\n",
    "            # plt.title('Dewarped Grayscale')\n",
    "            # plt.axis('off')\n",
    "            \n",
    "            #plt.subplot(1, 3, 3)\n",
    "            #plt.imshow(cv2_thresh_img_dewarped, cmap='gray')\n",
    "            #plt.title('Dewarped & Thresholded')\n",
    "            #plt.axis('off')\n",
    "            \n",
    "            #plt.show()\n",
    "    \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"ERROR: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "#Call page_assess_helper: SET THE PATH TO YOUR IMAGE FILE HERE\n",
    "def call_page_assess_helper():\n",
    "        \n",
    "    input_png_folder = \"PIL_to_png\" # Input Folder\n",
    "    output_dewarped_folder = \"dewarped\" # Output Folder Name\n",
    "    os.makedirs(output_dewarped_folder, exist_ok = True) # Create the folder\n",
    "\n",
    "    for chosen_png_file in os.listdir(input_png_folder):\n",
    "\n",
    "        input_png_path = os.path.join(input_png_folder, chosen_png_file) # create input png file path\n",
    "        # input_cv2_png_file = cv2.imread(input_png_path)   # Read png file\n",
    "        \n",
    "        cv2_thresh_img_dewarped = file_assess_helper(input_png_path)  # bw to no noise\n",
    "    \n",
    "        output_dewarped_path = os.path.join(output_dewarped_folder, f\"DEWARPED_{chosen_png_file}\")\n",
    "        cv2.imwrite(output_dewarped_path, cv2_thresh_img_dewarped)\n",
    "\n",
    "        # Display dewarpped image\n",
    "        # display_any(cv2_thresh_img_dewarped)\n",
    "    \n",
    "    \n",
    "    print(\"Dewarped file created\")\n",
    "    \n",
    "\n",
    "# Dewarp Starter\n",
    "# call_page_assess_helper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "735ac8af-3d97-4d30-b4a9-3d6f1f9d470d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1762370051536,
     "user": {
      "displayName": "Suman Gorkhali",
      "userId": "14082962432569903413"
     },
     "user_tz": 360
    },
    "id": "5cf06256-cf5f-4239-a909-c64cfd12eb16",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "4c5329e8-f545-4e5d-aad2-ce1956812233"
   },
   "outputs": [],
   "source": [
    "# Adaptive OCR Pipleline\n",
    "import pytesseract\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "class AdaptiveOCRPipeline:\n",
    "    \"\"\"\n",
    "    Automatically tests different preprocessing combinations and selects\n",
    "    the best one based on OCR confidence scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tesseract_cmd=None):\n",
    "        \"\"\"Initialize the adaptive pipeline.\"\"\"\n",
    "        if tesseract_cmd:\n",
    "            pytesseract.pytesseract.tesseract_cmd = tesseract_cmd\n",
    "    \n",
    "    # ========== OCR EVALUATION FUNCTION ==========\n",
    "    def evaluate_ocr_quality(self, image: np.ndarray) -> Tuple[float, str, int]:\n",
    "        \"\"\"\n",
    "        Evaluate OCR quality by running Tesseract and calculating confidence score.\n",
    "        \n",
    "        Args:\n",
    "            image: Preprocessed image (grayscale or binary)\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (average_confidence, extracted_text, char_count)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get detailed OCR data including confidence scores\n",
    "            data = pytesseract.image_to_data(image, output_type=pytesseract.Output.DICT)\n",
    "            \n",
    "            # Filter out low confidence and empty detections\n",
    "            confidences = [\n",
    "                float(conf) for conf, text in zip(data['conf'], data['text'])\n",
    "                if conf != '-1' and text.strip()\n",
    "            ]\n",
    "            \n",
    "            if not confidences:\n",
    "                return 0.0, \"\", 0\n",
    "            \n",
    "            # Calculate average confidence\n",
    "            avg_confidence = sum(confidences) / len(confidences)\n",
    "            \n",
    "            # Extract full text\n",
    "            text = pytesseract.image_to_string(image)\n",
    "            \n",
    "            # Additional quality metrics\n",
    "            word_count = len([w for w in text.split() if w.strip()])\n",
    "            char_count = len([c for c in text if c.isalnum()])\n",
    "            \n",
    "            # Penalize if very few characters detected (likely failed)\n",
    "            if char_count < 10:\n",
    "                avg_confidence *= 0.5\n",
    "            \n",
    "            # Bonus for reasonable amount of text\n",
    "            if char_count > 100 and word_count > 20:\n",
    "                avg_confidence *= 1.1\n",
    "            \n",
    "            return avg_confidence, text, char_count\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"OCR evaluation failed: {e}\")\n",
    "            return 0.0, \"\", 0\n",
    "\n",
    "    \n",
    "    \n",
    "    # ========== PIPELINE COMBINATIONS FUNCTION ==========\n",
    "    def create_pipeline_combinations(self) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Create different preprocessing pipeline combinations to test.\n",
    "        Uses your actual preprocessing functions from the main code.\n",
    "        \n",
    "        Returns:\n",
    "            List of pipeline dictionaries with 'name' and 'steps'\n",
    "        \"\"\"\n",
    "        pipelines = [\n",
    "            # Simple pipelines\n",
    "            {\n",
    "                'name': 'grayscale_only',\n",
    "                'steps': ['grayscale'],\n",
    "                'description': 'Just convert to grayscale'\n",
    "            },\n",
    "            {\n",
    "                'name': 'inverted_only',\n",
    "                'steps': ['inverted'],\n",
    "                'description': 'Just invert the image'\n",
    "            },\n",
    "            \n",
    "            # Grayscale + Binary pipelines\n",
    "            {\n",
    "                'name': 'gray_to_bw',\n",
    "                'steps': ['grayscale', 'bw'],\n",
    "                'description': 'Grayscale then black & white threshold'\n",
    "            },\n",
    "            {\n",
    "                'name': 'gray_bw_no_noise',\n",
    "                'steps': ['grayscale', 'bw', 'no_noise'],\n",
    "                'description': 'Grayscale + BW + noise removal'\n",
    "            },\n",
    "            \n",
    "            # With font modifications\n",
    "            {\n",
    "                'name': 'gray_bw_no_noise_thin',\n",
    "                'steps': ['grayscale', 'bw', 'no_noise', 'thin'],\n",
    "                'description': 'Standard pipeline with thin font'\n",
    "            },\n",
    "            {\n",
    "                'name': 'gray_bw_no_noise_thick',\n",
    "                'steps': ['grayscale', 'bw', 'no_noise', 'thick'],\n",
    "                'description': 'Standard pipeline with thick font'\n",
    "            },\n",
    "            \n",
    "            # With deskew\n",
    "            {\n",
    "                'name': 'deskew_gray_bw',\n",
    "                'steps': ['deskew', 'grayscale', 'bw'],\n",
    "                'description': 'Deskew first, then standard processing'\n",
    "            },\n",
    "            {\n",
    "                'name': 'deskew_gray_bw_no_noise',\n",
    "                'steps': ['deskew', 'grayscale', 'bw', 'no_noise'],\n",
    "                'description': 'Deskew + full standard pipeline'\n",
    "            },\n",
    "            \n",
    "            # With dewarp (most comprehensive)\n",
    "            {\n",
    "                'name': 'dewarp_only',\n",
    "                'steps': ['dewarp'],\n",
    "                'description': 'Just dewarp (already includes threshold)'\n",
    "            },\n",
    "            {\n",
    "                'name': 'dewarp_thin',\n",
    "                'steps': ['dewarp', 'thin'],\n",
    "                'description': 'Dewarp with thin font adjustment'\n",
    "            },\n",
    "            {\n",
    "                'name': 'dewarp_thick',\n",
    "                'steps': ['dewarp', 'thick'],\n",
    "                'description': 'Dewarp with thick font adjustment'\n",
    "            },\n",
    "            \n",
    "            # Inverted variants (for documents with dark backgrounds)\n",
    "            {\n",
    "                'name': 'inverted_gray_bw',\n",
    "                'steps': ['inverted', 'grayscale', 'bw'],\n",
    "                'description': 'Invert first for dark backgrounds'\n",
    "            },\n",
    "            {\n",
    "                'name': 'inverted_gray_bw_no_noise',\n",
    "                'steps': ['inverted', 'grayscale', 'bw', 'no_noise'],\n",
    "                'description': 'Inverted + full pipeline'\n",
    "            },\n",
    "        ]\n",
    "        \n",
    "        return pipelines\n",
    "    \n",
    "    # ========== APPLY PIPELINE FUNCTION ==========\n",
    "    def apply_pipeline(self, image_path: str, steps: List[str], \n",
    "                      temp_folder: str = \"temp_pipeline\") -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Apply a sequence of preprocessing steps to an image.\n",
    "        Uses your existing preprocessing functions.\n",
    "        \n",
    "        Args:\n",
    "            image_path: Path to input png file\n",
    "            steps: List of step names to apply\n",
    "            temp_folder: Temporary folder for intermediate files\n",
    "            \n",
    "        Returns:\n",
    "            Processed image as numpy array\n",
    "        \"\"\"\n",
    "        os.makedirs(temp_folder, exist_ok=True)\n",
    "        \n",
    "        # Read original image\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            raise ValueError(f\"Could not read image: {image_path}\")\n",
    "        \n",
    "        current_image = img.copy()\n",
    "        \n",
    "        for step in steps:\n",
    "            if step == 'grayscale':\n",
    "                current_image = png_to_gray(current_image)\n",
    "                \n",
    "            elif step == 'inverted':\n",
    "                current_image = cv2.bitwise_not(current_image)\n",
    "                \n",
    "            elif step == 'bw':\n",
    "                # Must be grayscale first\n",
    "                if len(current_image.shape) == 3:\n",
    "                    current_image = png_to_gray(current_image)\n",
    "                current_image = gray_to_bw(current_image)\n",
    "                \n",
    "            elif step == 'no_noise':\n",
    "                # Must be BW first\n",
    "                if len(current_image.shape) == 3:\n",
    "                    current_image = png_to_gray(current_image)\n",
    "                    current_image = gray_to_bw(current_image)\n",
    "                current_image = noise_removal(current_image)\n",
    "                \n",
    "            elif step == 'thin':\n",
    "                # Must be BW first\n",
    "                if len(current_image.shape) == 3:\n",
    "                    current_image = png_to_gray(current_image)\n",
    "                    current_image = gray_to_bw(current_image)\n",
    "                    current_image = noise_removal(current_image)\n",
    "                current_image = thin_font(current_image)\n",
    "                \n",
    "            elif step == 'thick':\n",
    "                # Must be BW first\n",
    "                if len(current_image.shape) == 3:\n",
    "                    current_image = png_to_gray(current_image)\n",
    "                    current_image = gray_to_bw(current_image)\n",
    "                    current_image = noise_removal(current_image)\n",
    "                current_image = thick_font(current_image)\n",
    "                \n",
    "            elif step == 'deskew':\n",
    "                current_image = deskewHybrid(current_image)\n",
    "                \n",
    "            elif step == 'dewarp':\n",
    "                # Dewarp returns thresholded binary image directly\n",
    "                current_image = file_assess_helper(image_path)\n",
    "                if current_image is None:\n",
    "                    print(f\"Warning: Dewarp failed, using original\")\n",
    "                    current_image = img\n",
    "                \n",
    "            else:\n",
    "                print(f\"Warning: Unknown step '{step}', skipping\")\n",
    "        \n",
    "        return current_image\n",
    "\n",
    "\n",
    "    # ========== FIND BEST PIPELINE FUNCTION ==========\n",
    "    def find_best_pipeline(self, image_path: str, \n",
    "                          custom_pipelines: List[Dict] = None,\n",
    "                          save_intermediates: bool = False,\n",
    "                          output_folder: str = \"pipeline_test_results\") -> Dict:\n",
    "        \n",
    "        \"\"\"    \n",
    "        Returns:\n",
    "            Dictionary containing:\n",
    "                - best_pipeline: Name of best pipeline\n",
    "                - best_steps: List of steps in best pipeline\n",
    "                - best_image: Best preprocessed image\n",
    "                - confidence: OCR confidence score\n",
    "                - text: Extracted text\n",
    "                - char_count: Number of characters detected\n",
    "                - all_results: Results from all pipelines tested\n",
    "        \"\"\"\n",
    "        if save_intermediates:\n",
    "            os.makedirs(output_folder, exist_ok=True)\n",
    "        \n",
    "        # Get pipeline combinations to test\n",
    "        pipelines = custom_pipelines or self.create_pipeline_combinations()\n",
    "        \n",
    "        results = []\n",
    "        best_score = 0\n",
    "        best_result = None\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Testing {len(pipelines)} preprocessing pipelines on:\")\n",
    "        print(f\"  {os.path.basename(image_path)}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        for i, pipeline in enumerate(pipelines, 1):\n",
    "            try:\n",
    "                print(f\"[{i}/{len(pipelines)}] Testing: {pipeline['name']}\")\n",
    "                print(f\"    Steps: {' â†’ '.join(pipeline['steps'])}\")\n",
    "                \n",
    "                # Apply preprocessing pipeline\n",
    "                processed = self.apply_pipeline(image_path, pipeline['steps'])\n",
    "                \n",
    "                # Evaluate OCR quality\n",
    "                confidence, text, char_count = self.evaluate_ocr_quality(processed)\n",
    "                \n",
    "                result = {\n",
    "                    'pipeline_name': pipeline['name'],\n",
    "                    'description': pipeline.get('description', ''),\n",
    "                    'steps': pipeline['steps'],\n",
    "                    'confidence': confidence,\n",
    "                    'text': text,\n",
    "                    'char_count': char_count,\n",
    "                    'image': processed\n",
    "                }\n",
    "                \n",
    "                results.append(result)\n",
    "                \n",
    "                print(f\"    âœ“ Confidence: {confidence:.2f} | Chars: {char_count}\")\n",
    "                print()\n",
    "                \n",
    "                # Track best result\n",
    "                if confidence > best_score:\n",
    "                    best_score = confidence\n",
    "                    best_result = result\n",
    "                \n",
    "                # Save intermediate results if requested\n",
    "                if save_intermediates:\n",
    "                    output_path = os.path.join(\n",
    "                        output_folder, \n",
    "                        f\"{i:02d}_{pipeline['name']}.png\"\n",
    "                    )\n",
    "                    cv2.imwrite(output_path, processed)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"    âœ— Pipeline {pipeline['name']} failed: {e}\\n\")\n",
    "        \n",
    "        if best_result is None:\n",
    "            print(\"\\nâŒ All pipelines failed!\")\n",
    "            return None\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ðŸ† BEST PIPELINE RESULTS\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Pipeline: {best_result['pipeline_name']}\")\n",
    "        print(f\"Steps: {' â†’ '.join(best_result['steps'])}\")\n",
    "        print(f\"Confidence Score: {best_result['confidence']:.2f}\")\n",
    "        print(f\"Characters Detected: {best_result['char_count']}\")\n",
    "        print(f\"\\nExtracted Text Preview (first 300 chars):\")\n",
    "        print(f\"{'-'*60}\")\n",
    "        print(best_result['text'][:300])\n",
    "        if len(best_result['text']) > 300:\n",
    "            print(\"...\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        # Show ranking of all pipelines\n",
    "        print(\"ðŸ“Š PIPELINE RANKING:\")\n",
    "        print(f\"{'Rank':<6} {'Pipeline':<30} {'Confidence':<12} {'Chars':<8}\")\n",
    "        print(f\"{'-'*60}\")\n",
    "        sorted_results = sorted(results, key=lambda x: x['confidence'], reverse=True)\n",
    "        for rank, r in enumerate(sorted_results, 1):\n",
    "            marker = \"ðŸ‘‘\" if rank == 1 else f\"{rank}.\"\n",
    "            print(f\"{marker:<6} {r['pipeline_name']:<30} {r['confidence']:<12.2f} {r['char_count']:<8}\")\n",
    "        print()\n",
    "        \n",
    "        return {\n",
    "            'best_pipeline': best_result['pipeline_name'],\n",
    "            'best_steps': best_result['steps'],\n",
    "            'best_image': best_result['image'],\n",
    "            'confidence': best_result['confidence'],\n",
    "            'text': best_result['text'],\n",
    "            'char_count': best_result['char_count'],\n",
    "            'all_results': results\n",
    "        }\n",
    "    \n",
    "    # ========== BATCH PROCESSING FUNCTION ==========\n",
    "    def process_folder(self, input_folder: str = \"PIL_to_png\",\n",
    "                      output_folder: str = \"adaptive_ocr_output\",\n",
    "                      save_best_images: bool = True,\n",
    "                      save_text_files: bool = True) -> Dict:\n",
    "      \n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        \n",
    "        # Get all png files\n",
    "        png_files = [f for f in os.listdir(input_folder) \n",
    "                    if f.lower().endswith('.png')]\n",
    "        \n",
    "        if not png_files:\n",
    "            print(f\"No png files found in {input_folder}\")\n",
    "            return {}\n",
    "        \n",
    "        print(f\"\\n{'#'*60}\")\n",
    "        print(f\"# ADAPTIVE OCR BATCH PROCESSING\")\n",
    "        print(f\"# Processing {len(png_files)} files from {input_folder}\")\n",
    "        print(f\"{'#'*60}\\n\")\n",
    "        \n",
    "        all_results = {}\n",
    "        \n",
    "        for i, filename in enumerate(png_files, 1):\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"FILE {i}/{len(png_files)}: {filename}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            input_path = os.path.join(input_folder, filename)\n",
    "            \n",
    "            # Find best pipeline for this image\n",
    "            result = self.find_best_pipeline(\n",
    "                input_path,\n",
    "                save_intermediates=False\n",
    "            )\n",
    "            \n",
    "            if result:\n",
    "                all_results[filename] = result\n",
    "                \n",
    "                # Save best preprocessed image\n",
    "                if save_best_images:\n",
    "                    image_output = os.path.join(\n",
    "                        output_folder,\n",
    "                        f\"BEST_{filename}\"\n",
    "                    )\n",
    "                    cv2.imwrite(image_output, result['best_image'])\n",
    "                    print(f\"ðŸ’¾ Saved best image: {image_output}\")\n",
    "                \n",
    "                # Save extracted text\n",
    "                if save_text_files:\n",
    "                    text_output = os.path.join(\n",
    "                        output_folder,\n",
    "                        f\"{os.path.splitext(filename)[0]}.txt\"\n",
    "                    )\n",
    "                    with open(text_output, 'w', encoding='utf-8') as f:\n",
    "                        f.write(f\"Pipeline: {result['best_pipeline']}\\n\")\n",
    "                        f.write(f\"Confidence: {result['confidence']:.2f}\\n\")\n",
    "                        f.write(f\"Characters: {result['char_count']}\\n\")\n",
    "                        f.write(f\"{'-'*60}\\n\\n\")\n",
    "                        f.write(result['text'])\n",
    "                    print(f\"ðŸ’¾ Saved text: {text_output}\")\n",
    "        \n",
    "        \n",
    "        print(f\"\\n{'#'*60}\")\n",
    "        print(f\"# BATCH PROCESSING COMPLETE\")\n",
    "        print(f\"# Processed: {len(all_results)}/{len(png_files)} files\")\n",
    "        print(f\"# Results saved to: {output_folder}\")\n",
    "        print(f\"{'#'*60}\\n\")\n",
    "        \n",
    "        return all_results\n",
    "\n",
    "\n",
    "def apply_pipeline_safe(self, image_path: str, steps: List[str], \n",
    "                       temp_folder: str = \"temp_pipeline\") -> np.ndarray:\n",
    "    \n",
    "    os.makedirs(temp_folder, exist_ok=True)\n",
    "    \n",
    "    # Read original image\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Could not read image: {image_path}\")\n",
    "    \n",
    "    current_image = img.copy()\n",
    "    \n",
    "    for step in steps:\n",
    "        try:\n",
    "            if step == 'grayscale':\n",
    "                current_image = png_to_gray(current_image)\n",
    "                \n",
    "            elif step == 'inverted':\n",
    "                current_image = png_to_inverted(current_image)\n",
    "                \n",
    "            elif step == 'bw':\n",
    "                # Must be grayscale first\n",
    "                if len(current_image.shape) == 3:\n",
    "                    current_image = png_to_gray(current_image)\n",
    "                current_image = gray_to_bw(current_image)\n",
    "                \n",
    "            elif step == 'no_noise':\n",
    "                # Must be BW first\n",
    "                if len(current_image.shape) == 3:\n",
    "                    current_image = png_to_gray(current_image)\n",
    "                    current_image = gray_to_bw(current_image)\n",
    "                current_image = noise_removal(current_image)\n",
    "                \n",
    "            elif step == 'thin':\n",
    "                # Must be BW first\n",
    "                if len(current_image.shape) == 3:\n",
    "                    current_image = png_to_gray(current_image)\n",
    "                    current_image = gray_to_bw(current_image)\n",
    "                    current_image = noise_removal(current_image)\n",
    "                current_image = thin_font(current_image)\n",
    "                \n",
    "            elif step == 'thick':\n",
    "                # Must be BW first\n",
    "                if len(current_image.shape) == 3:\n",
    "                    current_image = png_to_gray(current_image)\n",
    "                    current_image = gray_to_bw(current_image)\n",
    "                    current_image = noise_removal(current_image)\n",
    "                current_image = thick_font(current_image)\n",
    "                \n",
    "            elif step == 'deskew':\n",
    "                current_image = deskewHybrid(current_image)\n",
    "                \n",
    "            elif step == 'dewarp':\n",
    "                dewarped = file_assess_helper(image_path)\n",
    "                if dewarped is not None:\n",
    "                    current_image = dewarped\n",
    "                else:\n",
    "                    print(f\"    âš ï¸  Dewarp returned None, skipping this step\")\n",
    "                    # Continue with current_image unchanged\n",
    "                \n",
    "            else:\n",
    "                print(f\"    âš ï¸  Unknown step '{step}', skipping\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"    âš ï¸  Step '{step}' failed: {str(e)[:50]}... Skipping.\")\n",
    "            # Continue with current_image unchanged\n",
    "    \n",
    "    return current_image\n",
    "\n",
    "\n",
    "# ========== DISABLE AUTO-EXECUTION ==========\n",
    "# For Future improvement\n",
    "def manual_run_all(pdf_path, manual):\n",
    "    \n",
    "    # RUN_INDIVIDUAL_STEPS = False  # Set to True only when you want to run individual steps\n",
    "    RUN_INDIVIDUAL_STEPS = manual\n",
    "    \n",
    "    if RUN_INDIVIDUAL_STEPS:\n",
    "        call_PIL_to_png_save()\n",
    "        call_png_to_inverted()\n",
    "        call_png_to_gray()\n",
    "        call_gray_to_bw()\n",
    "        call_noise_removal()\n",
    "        call_thin_font()\n",
    "        call_thick_font()\n",
    "        call_deskewHybrid()\n",
    "        call_page_assess_helper()\n",
    "\n",
    "\n",
    "\n",
    "# ========== PROGRESS TRACKING ==========\n",
    "\n",
    "def find_best_pipeline_with_progress(self, image_path: str, \n",
    "                                    custom_pipelines: List[Dict] = None,\n",
    "                                    save_intermediates: bool = False,\n",
    "                                    output_folder: str = \"pipeline_test_results\") -> Dict:\n",
    "    \n",
    "    import time\n",
    "    \n",
    "    if save_intermediates:\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    pipelines = custom_pipelines or self.create_pipeline_combinations()\n",
    "    \n",
    "    results = []\n",
    "    best_score = 0\n",
    "    best_result = None\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ðŸ” TESTING {len(pipelines)} PREPROCESSING PIPELINES\")\n",
    "    print(f\"ðŸ“„ Image: {os.path.basename(image_path)}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    for i, pipeline in enumerate(pipelines, 1):\n",
    "        try:\n",
    "            step_start_time = time.time()\n",
    "            \n",
    "            print(f\"[{i:2d}/{len(pipelines)}] {pipeline['name']:<30}\", end=\" \", flush=True)\n",
    "            \n",
    "            # Apply preprocessing pipeline\n",
    "            processed = self.apply_pipeline(image_path, pipeline['steps'])\n",
    "            \n",
    "            # Evaluate OCR quality\n",
    "            confidence, text, char_count = self.evaluate_ocr_quality(processed)\n",
    "            \n",
    "            step_time = time.time() - step_start_time\n",
    "            \n",
    "            result = {\n",
    "                'pipeline_name': pipeline['name'],\n",
    "                'description': pipeline.get('description', ''),\n",
    "                'steps': pipeline['steps'],\n",
    "                'confidence': confidence,\n",
    "                'text': text,\n",
    "                'char_count': char_count,\n",
    "                'image': processed,\n",
    "                'time_seconds': step_time\n",
    "            }\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "            # Show result inline\n",
    "            print(f\"âœ“ Score: {confidence:5.1f} | Chars: {char_count:5d} | Time: {step_time:4.1f}s\")\n",
    "            \n",
    "            # Track best result\n",
    "            if confidence > best_score:\n",
    "                best_score = confidence\n",
    "                best_result = result\n",
    "            \n",
    "            # Save intermediate results if requested\n",
    "            if save_intermediates:\n",
    "                output_path = os.path.join(\n",
    "                    output_folder, \n",
    "                    f\"{i:02d}_{pipeline['name']}.png\"\n",
    "                )\n",
    "                cv2.imwrite(output_path, processed)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âœ— FAILED: {str(e)[:40]}...\")\n",
    "    \n",
    "    total_time = time.time() - total_start_time\n",
    "    \n",
    "    if best_result is None:\n",
    "        print(\"\\nâŒ All pipelines failed!\")\n",
    "        return None\n",
    "    \n",
    "    # Print detailed summary\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ðŸ† BEST PIPELINE RESULTS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Pipeline:    {best_result['pipeline_name']}\")\n",
    "    print(f\"Steps:       {' â†’ '.join(best_result['steps'])}\")\n",
    "    print(f\"Confidence:  {best_result['confidence']:.2f}\")\n",
    "    print(f\"Characters:  {best_result['char_count']}\")\n",
    "    print(f\"Time:        {best_result['time_seconds']:.2f}s\")\n",
    "    print(f\"Total Time:  {total_time:.2f}s\")\n",
    "    print(f\"\\nðŸ“ Extracted Text Preview (first 200 chars):\")\n",
    "    print(f\"{'-'*70}\")\n",
    "    print(best_result['text'][:200].strip())\n",
    "    if len(best_result['text']) > 200:\n",
    "        print(\"...\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Show top 5 pipelines\n",
    "    print(\"ðŸ“Š TOP 5 PIPELINES:\")\n",
    "    print(f\"{'Rank':<6} {'Pipeline':<30} {'Score':<8} {'Chars':<8} {'Time':<8}\")\n",
    "    print(f\"{'-'*70}\")\n",
    "    sorted_results = sorted(results, key=lambda x: x['confidence'], reverse=True)\n",
    "    for rank, r in enumerate(sorted_results[:5], 1):\n",
    "        marker = \"ðŸ¥‡\" if rank == 1 else \"ðŸ¥ˆ\" if rank == 2 else \"ðŸ¥‰\" if rank == 3 else f\"{rank}.\"\n",
    "        print(f\"{marker:<6} {r['pipeline_name']:<30} {r['confidence']:<8.1f} \"\n",
    "              f\"{r['char_count']:<8} {r['time_seconds']:<8.2f}s\")\n",
    "    print()\n",
    "    \n",
    "    return {\n",
    "        'best_pipeline': best_result['pipeline_name'],\n",
    "        'best_steps': best_result['steps'],\n",
    "        'best_image': best_result['image'],\n",
    "        'confidence': best_result['confidence'],\n",
    "        'text': best_result['text'],\n",
    "        'char_count': best_result['char_count'],\n",
    "        'processing_time': best_result['time_seconds'],\n",
    "        'total_time': total_time,\n",
    "        'all_results': results\n",
    "    }\n",
    "\n",
    "\n",
    "# ========== QUICK TEST FUNCTION ==========\n",
    "\n",
    "def quick_test_on_single_page():\n",
    "    \"\"\"\n",
    "    Quick test function to verify everything works on one page.\n",
    "    Run this first before processing the entire folder.\n",
    "    \"\"\"\n",
    "    print(\"ðŸš€ QUICK TEST - Single Page\\n\")\n",
    "    \n",
    "    # Check if PIL_to_png folder exists\n",
    "    if not os.path.exists('PIL_to_png'):\n",
    "        print(\"âŒ Error: 'PIL_to_png' folder not found!\")\n",
    "        print(\"   Please run call_PIL_to_png_save() first to convert your PDF.\")\n",
    "        return\n",
    "    \n",
    "    # Get first png file\n",
    "    png_files = [f for f in os.listdir('PIL_to_png') if f.endswith('.png')]\n",
    "    if not png_files:\n",
    "        print(\"âŒ Error: No png files found in 'PIL_to_png' folder!\")\n",
    "        return\n",
    "    \n",
    "    test_file = os.path.join('PIL_to_png', png_files[0])\n",
    "    print(f\"ðŸ“„ Testing on: {png_files[0]}\\n\")\n",
    "    \n",
    "    # Initialize and test\n",
    "    pipeline = AdaptiveOCRPipeline()\n",
    "    result = pipeline.find_best_pipeline(\n",
    "        test_file,\n",
    "        save_intermediates=True,\n",
    "        output_folder='test_results'\n",
    "    )\n",
    "    \n",
    "    if result:\n",
    "        print(\"\\nâœ… SUCCESS! The pipeline is working correctly.\")\n",
    "        print(f\"   Best method: {result['best_pipeline']}\")\n",
    "        print(f\"   Check 'test_results' folder for all tested versions.\")\n",
    "    else:\n",
    "        print(\"\\nâŒ Something went wrong. Check error messages above.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9283aac7-5356-4f6b-9e78-61a3a255e6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTOMATED OLLAMA DATA EXTRACTION TO CSV\n",
    "## Add these functions to your code\n",
    "\n",
    "import ollama\n",
    "import csv\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "\n",
    "# ========== OLLAMA DATA EXTRACTION CLASS ==========\n",
    "\n",
    "class OllamaDataExtractor:\n",
    "    \"\"\"\n",
    "    Extracts structured data from OCR text using Ollama model.\n",
    "    Saves results to CSV format for Excel import.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='ocranalyst:latest'):\n",
    "        \"\"\"\n",
    "        Initialize the extractor.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Name of your Ollama model\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.client = ollama.Client()\n",
    "        \n",
    "        # Define the fields to extract\n",
    "        self.fields = [\n",
    "            'No.',\n",
    "            'Instrument Type Hyperlink',\n",
    "            'BOOK TYPE',\n",
    "            'Inst. Number',\n",
    "            'Vol.',\n",
    "            'Pg.',\n",
    "            'Inst. Date',\n",
    "            'File Date',\n",
    "            'Grantor',\n",
    "            'Grantee',\n",
    "            'Legal',\n",
    "            'Comments'\n",
    "        ]\n",
    "        \n",
    "        print(f\"âœ… Ollama Data Extractor initialized with model: {model_name}\")\n",
    "    \n",
    "    def test_ollama_connection(self) -> bool:\n",
    "        \"\"\"\n",
    "        Test if Ollama is running and model is available.\n",
    "        \n",
    "        Returns:\n",
    "            True if connection successful, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Try a simple prompt to test connection\n",
    "            response = self.client.generate(  # âœ… CORRECT - This is Ollama format\n",
    "                model=self.model_name,\n",
    "                prompt=\"Test connection. Respond with 'OK'.\",\n",
    "                stream=False\n",
    "            )\n",
    "            \n",
    "            print(f\"âœ… Ollama connection successful\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Ollama connection failed: {e}\")\n",
    "            print(f\"   Make sure Ollama is running: 'ollama serve'\")\n",
    "            print(f\"   And model is available: 'ollama list'\")\n",
    "            return False\n",
    "    \n",
    "    def create_extraction_prompt(self, ocr_text: str, document_number: int) -> str:\n",
    "        \"\"\"\n",
    "        Create a detailed prompt for the Ollama model.\n",
    "        \n",
    "        Args:\n",
    "            ocr_text: The OCR extracted text\n",
    "            document_number: Sequential number for this document\n",
    "            \n",
    "        Returns:\n",
    "            Formatted prompt string\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"You are a legal contract document data extraction specialist. Extract the following information from the OCR text below and return it as JSON.\n",
    "\n",
    "REQUIRED FIELDS TO EXTRACT:\n",
    "1. No. - Use this number: {document_number}\n",
    "2. Instrument Type Hyperlink - Type of legal document (e.g., \"Power of Attorney\", \"Certified Copy of Patent\", \"Deed\", \"Mortgage\", \"Warranty Deed\")\n",
    "3. BOOK TYPE - Type of record book (e.g., \"Deed Records\", \"Mortgage Records\")\n",
    "4. Inst. Number - Instrument number if mentioned\n",
    "5. Vol. - Volume number (format: 000E, 001A, etc.)\n",
    "6. Pg. - Page number\n",
    "7. Inst. Date - Instrument date (format: MM/DD/YYYY)\n",
    "8. File Date - Filing date (format: MM-DD-YYYY or MM/DD/YYYY)\n",
    "9. Grantor - Person/entity granting (seller, attorney)\n",
    "10. Grantee - Person/entity receiving (buyer, beneficiary)\n",
    "11. Legal - Legal description of property (if any)\n",
    "12. Comments - Summary of the document's purpose and key details\n",
    "\n",
    "IMPORTANT INSTRUCTIONS:\n",
    "- If a field is not found, use an empty string \"\"\n",
    "- For dates, use the format shown in the examples\n",
    "- For Comments, provide a clear, concise summary of the document\n",
    "- Return ONLY valid JSON, no other text\n",
    "- Use double quotes for strings\n",
    "\n",
    "EXAMPLE OUTPUT FORMAT:\n",
    "{{\n",
    "    \"No.\": \"1\",\n",
    "    \"Instrument Type Hyperlink\": \"Power of Attorney\",\n",
    "    \"BOOK TYPE\": \"Deed Records\",\n",
    "    \"Inst. Number\": \"\",\n",
    "    \"Vol.\": \"000E\",\n",
    "    \"Pg.\": \"133\",\n",
    "    \"Inst. Date\": \"11/1/1838\",\n",
    "    \"File Date\": \"11/11/1838\",\n",
    "    \"Grantor\": \"Joseph Simms\",\n",
    "    \"Grantee\": \"Elisha Roberts\",\n",
    "    \"Legal\": \"\",\n",
    "    \"Comments\": \"Appoint as my true and lawful attorney in fact for me and in my name to sell and execute for me on Chinquapin Bayou where I now live, on the E side of Ayish Bayou and for me and in my name.\"\n",
    "}}\n",
    "\n",
    "OCR TEXT TO ANALYZE:\n",
    "{ocr_text}\n",
    "\n",
    "Extract the data and return ONLY the JSON object:\"\"\"\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def extract_data_from_text(self, ocr_text: str, document_number: int, \n",
    "                               filename: str) -> Optional[Dict]:\n",
    "        \"\"\"\n",
    "        Extract structured data from OCR text using Ollama.\n",
    "        \n",
    "        Args:\n",
    "            ocr_text: The OCR extracted text\n",
    "            document_number: Sequential number for this document\n",
    "            filename: Name of the source file\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with extracted data or None if extraction failed\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"  ðŸ¤– Analyzing with Ollama model: {filename}\")\n",
    "            \n",
    "            # Create prompt\n",
    "            prompt = self.create_extraction_prompt(ocr_text, document_number)\n",
    "            \n",
    "            # Call Ollama model\n",
    "            response = self.client.generate(\n",
    "                model=self.model_name,\n",
    "                prompt=prompt,\n",
    "                stream=False,\n",
    "                think=False\n",
    "            )\n",
    "            \n",
    "            # Extract response text\n",
    "            response_text = response['response']\n",
    "            \n",
    "            # Parse JSON from response\n",
    "            # Sometimes models wrap JSON in markdown, so clean it\n",
    "            response_text = response_text.strip()\n",
    "            \n",
    "            # Remove markdown code blocks if present\n",
    "            if response_text.startswith('```'):\n",
    "                response_text = re.sub(r'^```(?:json)?\\s*', '', response_text)\n",
    "                response_text = re.sub(r'\\s*```$', '', response_text)\n",
    "            \n",
    "            # Parse JSON\n",
    "            extracted_data = json.loads(response_text)\n",
    "            \n",
    "            print(f\"  âœ… Extracted data successfully\")\n",
    "            return extracted_data\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"  âš ï¸  Failed to parse JSON response: {e}\")\n",
    "            print(f\"      Raw response: {response_text[:200]}...\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ Extraction failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def process_ocr_results_folder(self, ocr_folder: str = 'adaptive_ocr_results',\n",
    "                                   output_csv: str = 'extracted_data.csv') -> bool:\n",
    "        \"\"\"\n",
    "        Process all OCR text files in the folder and extract data.\n",
    "        \n",
    "        Args:\n",
    "            ocr_folder: Folder containing OCR results\n",
    "            output_csv: Output CSV filename\n",
    "            \n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"ðŸ“Š OLLAMA DATA EXTRACTION TO CSV\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Test Ollama connection first\n",
    "        if not self.test_ollama_connection():\n",
    "            return False\n",
    "        \n",
    "        # Get all text files\n",
    "        if not os.path.exists(ocr_folder):\n",
    "            print(f\"âŒ Folder not found: {ocr_folder}\")\n",
    "            return False\n",
    "        \n",
    "        txt_files = sorted([f for f in os.listdir(ocr_folder) if f.endswith('.txt')])\n",
    "        \n",
    "        if not txt_files:\n",
    "            print(f\"âŒ No .txt files found in {ocr_folder}\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"ðŸ“„ Found {len(txt_files)} text files to process\")\n",
    "        print(f\"ðŸŽ¯ Output CSV: {output_csv}\\n\")\n",
    "        \n",
    "        # Process each file\n",
    "        all_extracted_data = []\n",
    "        \n",
    "        for idx, txt_file in enumerate(txt_files, start=1):\n",
    "            print(f\"[{idx}/{len(txt_files)}] Processing: {txt_file}\")\n",
    "            \n",
    "            # Read OCR text\n",
    "            txt_path = os.path.join(ocr_folder, txt_file)\n",
    "            try:\n",
    "                with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "                    lines = f.readlines()\n",
    "                    \n",
    "                    # Skip the header lines (Pipeline, Confidence, Characters, ---)\n",
    "                    # Get only the actual OCR text\n",
    "                    if len(lines) > 4:\n",
    "                        ocr_text = ''.join(lines[4:])\n",
    "                    else:\n",
    "                        ocr_text = ''.join(lines)\n",
    "                \n",
    "                # Extract data using Ollama\n",
    "                extracted_data = self.extract_data_from_text(\n",
    "                    ocr_text=ocr_text,\n",
    "                    document_number=idx,\n",
    "                    filename=txt_file\n",
    "                )\n",
    "                \n",
    "                if extracted_data:\n",
    "                    all_extracted_data.append(extracted_data)\n",
    "                else:\n",
    "                    # Create empty record if extraction failed\n",
    "                    print(f\"  âš ï¸  Creating empty record for {txt_file}\")\n",
    "                    empty_record = {field: \"\" for field in self.fields}\n",
    "                    empty_record['No.'] = str(idx)\n",
    "                    all_extracted_data.append(empty_record)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  âŒ Error reading file: {e}\")\n",
    "                # Create empty record\n",
    "                empty_record = {field: \"\" for field in self.fields}\n",
    "                empty_record['No.'] = str(idx)\n",
    "                all_extracted_data.append(empty_record)\n",
    "            \n",
    "            print()\n",
    "        \n",
    "        # Save to CSV\n",
    "        if all_extracted_data:\n",
    "            success = self.save_to_csv(all_extracted_data, output_csv)\n",
    "            if success:\n",
    "                print(f\"\\n{'='*70}\")\n",
    "                print(f\"âœ… DATA EXTRACTION COMPLETE\")\n",
    "                print(f\"{'='*70}\")\n",
    "                print(f\"ðŸ“Š Total records: {len(all_extracted_data)}\")\n",
    "                print(f\"ðŸ’¾ CSV saved to: {output_csv}\")\n",
    "                print(f\"ðŸ“‚ Open in Excel: {os.path.abspath(output_csv)}\")\n",
    "                print(f\"{'='*70}\\n\")\n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def save_to_csv(self, data: List[Dict], output_csv: str) -> bool:\n",
    "        \"\"\"\n",
    "        Save extracted data to CSV file.\n",
    "        \n",
    "        Args:\n",
    "            data: List of dictionaries with extracted data\n",
    "            output_csv: Output CSV filename\n",
    "            \n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=self.fields)\n",
    "                \n",
    "                # Write header\n",
    "                writer.writeheader()\n",
    "                \n",
    "                # Write data rows\n",
    "                for record in data:\n",
    "                    # Ensure all fields exist in the record\n",
    "                    row = {field: record.get(field, \"\") for field in self.fields}\n",
    "                    writer.writerow(row)\n",
    "            \n",
    "            print(f\"  ðŸ’¾ CSV saved successfully: {output_csv}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ Error saving CSV: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def create_sample_csv(self, output_csv: str = 'sample_extracted_data.csv'):\n",
    "        \"\"\"\n",
    "        Create a sample CSV with your example data for testing.\n",
    "        \"\"\"\n",
    "        sample_data = [\n",
    "            {\n",
    "                'No.': '1',\n",
    "                'Instrument Type Hyperlink': 'Power of Attorney',\n",
    "                'BOOK TYPE': 'Deed Records',\n",
    "                'Inst. Number': '',\n",
    "                'Vol.': '000E',\n",
    "                'Pg.': '133',\n",
    "                'Inst. Date': '11/1/1838',\n",
    "                'File Date': '11/11/1838',\n",
    "                'Grantor': 'Joseph Simms',\n",
    "                'Grantee': 'Elisha Roberts',\n",
    "                'Legal': '',\n",
    "                'Comments': 'Appoint as my true and lawful attorney in fact for me and in my name to sell and execute for me on Chinquapin Bayou where I now live, on the E side of Ayish Bayou and for me and in my name.'\n",
    "            },\n",
    "            {\n",
    "                'No.': '2',\n",
    "                'Instrument Type Hyperlink': 'Certified Copy of Patent',\n",
    "                'BOOK TYPE': 'Deed Records',\n",
    "                'Inst. Number': '3615',\n",
    "                'Vol.': '0099',\n",
    "                'Pg.': '508',\n",
    "                'Inst. Date': '5/1/1841',\n",
    "                'File Date': '11-28-1947',\n",
    "                'Grantor': 'State of Texas',\n",
    "                'Grantee': 'Joseph Simms',\n",
    "                'Legal': '',\n",
    "                'Comments': 'State of Texas grants One Labor or One Million Square varas of land to Joseph Simms and his heirs and assigns forever. ARTI NMR M&B Patent was refilled in San Augustine County on 11/28/1947.'\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        self.save_to_csv(sample_data, output_csv)\n",
    "        print(f\"âœ… Sample CSV created: {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8bc24bba-6d9b-4ebc-ba70-df8fe887e927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== image_document_pre_processing ==========\n",
    "\n",
    "def image_document_pre_processing(pdf_path, save):\n",
    "\n",
    "    \"\"\"\n",
    "    Main function to process all PNG files with adaptive OCR.\n",
    "    This is the function that was missing from your code.\n",
    "    \"\"\"\n",
    "\n",
    "    # Call to Create png files\n",
    "    if save:\n",
    "         manual_run_all(pdf_path)\n",
    "\n",
    "    # Initialize the adaptive pipeline\n",
    "    pipeline = AdaptiveOCRPipeline()\n",
    "    \n",
    "    # Process entire folder - FIXED: Using correct folder name\n",
    "    results = pipeline.process_folder(\n",
    "        input_folder='PIL_to_png',  \n",
    "        output_folder='adaptive_ocr_results',\n",
    "        save_best_images=True,\n",
    "        save_text_files=True\n",
    "    )\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nðŸ“ˆ OVERALL STATISTICS:\")\n",
    "    \n",
    "    \n",
    "    if results:\n",
    "        avg_confidence = sum(r['confidence'] for r in results.values()) / len(results)\n",
    "        total_chars = sum(r['char_count'] for r in results.values())\n",
    "        print(f\"Average Confidence: {avg_confidence:.2f}\")\n",
    "        print(f\"Total Characters: {total_chars}\")\n",
    "        \n",
    "        # Show which pipeline was best for each page\n",
    "        print(f\"\\nðŸ“Š PIPELINE USAGE SUMMARY:\")\n",
    "        from collections import Counter\n",
    "        pipeline_counts = Counter(r['best_pipeline'] for r in results.values())\n",
    "        for pipeline_name, count in pipeline_counts.most_common():\n",
    "            print(f\"  {pipeline_name}: {count} pages\")\n",
    "\n",
    "    else:\n",
    "        print(\"  âŒ No OCR results generated\")\n",
    "        return\n",
    "    \n",
    "    # Step 2: Extract structured data using Ollama\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"STEP 2: DATA EXTRACTION WITH OLLAMA\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    extractor = OllamaDataExtractor(model_name='ocranalyst:latest')\n",
    "    \n",
    "    # Process OCR results and create CSV\n",
    "    success = extractor.process_ocr_results_folder(\n",
    "        ocr_folder='adaptive_ocr_results',\n",
    "        output_csv='extracted_data.csv'\n",
    "    )\n",
    "    \n",
    "    if success:\n",
    "        print(\"âœ… Complete pipeline finished successfully!\")\n",
    "        print(f\"   1. OCR results saved in: adaptive_ocr_results/\")\n",
    "        print(f\"   2. Extracted data saved in: extracted_data.csv\")\n",
    "        print(f\"\\nðŸ’¡ You can now open extracted_data.csv in Microsoft Excel\")\n",
    "    else:\n",
    "        print(\"âš ï¸  Data extraction failed, but OCR results are available\")\n",
    "        print(f\"   Check adaptive_ocr_results/ folder for text files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "303d9b67-7286-463b-9f27-b95fb95aa11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Ollama Data Extractor initialized with model: ocranalyst:latest\n",
      "\n",
      "======================================================================\n",
      "ðŸ“Š OLLAMA DATA EXTRACTION TO CSV\n",
      "======================================================================\n",
      "âœ… Ollama connection successful\n",
      "ðŸ“„ Found 2 text files to process\n",
      "ðŸŽ¯ Output CSV: extracted_data.csv\n",
      "\n",
      "[1/2] Processing: page_1.txt\n",
      "  ðŸ¤– Analyzing with Ollama model: page_1.txt\n",
      "  âœ… Extracted data successfully\n",
      "\n",
      "[2/2] Processing: page_2.txt\n",
      "  ðŸ¤– Analyzing with Ollama model: page_2.txt\n",
      "  âœ… Extracted data successfully\n",
      "\n",
      "  ðŸ’¾ CSV saved successfully: extracted_data.csv\n",
      "\n",
      "======================================================================\n",
      "âœ… DATA EXTRACTION COMPLETE\n",
      "======================================================================\n",
      "ðŸ“Š Total records: 2\n",
      "ðŸ’¾ CSV saved to: extracted_data.csv\n",
      "ðŸ“‚ Open in Excel: /Users/m2mba2022/Documents/Fall2025/GraduateProject/from server/qwen3_14b/extracted_data.csv\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def qwen_data_extractor():\n",
    "    extractor = OllamaDataExtractor(model_name='ocranalyst:latest')\n",
    "    \n",
    "    # Process OCR results and create CSV\n",
    "    success = extractor.process_ocr_results_folder(\n",
    "        ocr_folder='adaptive_ocr_results',\n",
    "        output_csv='extracted_data.csv'\n",
    "    )\n",
    "\n",
    "qwen_data_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d1f22fe6-fa70-4031-b183-1935912e6d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸ“š DOCUMENT PREPROCESSING & ADAPTIVE OCR SYSTEM\n",
      "======================================================================\n",
      "\n",
      "2 REGULAR png files from pdf saved\n",
      "\n",
      "############################################################\n",
      "# ADAPTIVE OCR BATCH PROCESSING\n",
      "# Processing 2 files from PIL_to_png\n",
      "############################################################\n",
      "\n",
      "\n",
      "============================================================\n",
      "FILE 1/2: page_2.png\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Testing 13 preprocessing pipelines on:\n",
      "  page_2.png\n",
      "============================================================\n",
      "\n",
      "[1/13] Testing: grayscale_only\n",
      "    Steps: grayscale\n",
      "    âœ“ Confidence: 84.37 | Chars: 2200\n",
      "\n",
      "[2/13] Testing: inverted_only\n",
      "    Steps: inverted\n",
      "    âœ“ Confidence: 83.81 | Chars: 2210\n",
      "\n",
      "[3/13] Testing: gray_to_bw\n",
      "    Steps: grayscale â†’ bw\n",
      "    âœ“ Confidence: 80.50 | Chars: 2217\n",
      "\n",
      "[4/13] Testing: gray_bw_no_noise\n",
      "    Steps: grayscale â†’ bw â†’ no_noise\n",
      "    âœ“ Confidence: 46.60 | Chars: 772\n",
      "\n",
      "[5/13] Testing: gray_bw_no_noise_thin\n",
      "    Steps: grayscale â†’ bw â†’ no_noise â†’ thin\n",
      "    âœ“ Confidence: 42.83 | Chars: 604\n",
      "\n",
      "[6/13] Testing: gray_bw_no_noise_thick\n",
      "    Steps: grayscale â†’ bw â†’ no_noise â†’ thick\n",
      "    âœ“ Confidence: 43.04 | Chars: 1049\n",
      "\n",
      "[7/13] Testing: deskew_gray_bw\n",
      "    Steps: deskew â†’ grayscale â†’ bw\n",
      "    âœ“ Confidence: 80.24 | Chars: 2423\n",
      "\n",
      "[8/13] Testing: deskew_gray_bw_no_noise\n",
      "    Steps: deskew â†’ grayscale â†’ bw â†’ no_noise\n",
      "    âœ“ Confidence: 45.25 | Chars: 1143\n",
      "\n",
      "[9/13] Testing: dewarp_only\n",
      "    Steps: dewarp\n",
      "Got 42 spans with 394 points.\n",
      "Initial objective is 0.07218784511990275\n",
      "Optimizing 444 parameters...\n",
      "Optimization took 3.89 sec.\n",
      "Final objective is 0.007152157268433242\n",
      "Got page dims 0.8426745833272766 x 1.8348646994982636\n",
      "Output will be 944x2032\n",
      "    âœ“ Confidence: 81.84 | Chars: 2277\n",
      "\n",
      "[10/13] Testing: dewarp_thin\n",
      "    Steps: dewarp â†’ thin\n",
      "Got 42 spans with 394 points.\n",
      "Initial objective is 0.07218784511990275\n",
      "Optimizing 444 parameters...\n",
      "Optimization took 3.89 sec.\n",
      "Final objective is 0.007152157268433242\n",
      "Got page dims 0.8426745833272766 x 1.8348646994982636\n",
      "Output will be 944x2032\n",
      "    âœ“ Confidence: 73.14 | Chars: 2275\n",
      "\n",
      "[11/13] Testing: dewarp_thick\n",
      "    Steps: dewarp â†’ thick\n",
      "Got 42 spans with 394 points.\n",
      "Initial objective is 0.07218784511990275\n",
      "Optimizing 444 parameters...\n",
      "Optimization took 3.89 sec.\n",
      "Final objective is 0.007152157268433242\n",
      "Got page dims 0.8426745833272766 x 1.8348646994982636\n",
      "Output will be 944x2032\n",
      "    âœ“ Confidence: 74.49 | Chars: 2284\n",
      "\n",
      "[12/13] Testing: inverted_gray_bw\n",
      "    Steps: inverted â†’ grayscale â†’ bw\n",
      "    âœ“ Confidence: 76.95 | Chars: 1455\n",
      "\n",
      "[13/13] Testing: inverted_gray_bw_no_noise\n",
      "    Steps: inverted â†’ grayscale â†’ bw â†’ no_noise\n",
      "    âœ“ Confidence: 65.91 | Chars: 1780\n",
      "\n",
      "\n",
      "============================================================\n",
      "ðŸ† BEST PIPELINE RESULTS\n",
      "============================================================\n",
      "Pipeline: grayscale_only\n",
      "Steps: grayscale\n",
      "Confidence Score: 84.37\n",
      "Characters Detected: 2200\n",
      "\n",
      "Extracted Text Preview (first 300 chars):\n",
      "------------------------------------------------------------\n",
      "amendÂ»\n",
      "\n",
      "1, or otherwig\n",
      "8\n",
      "\n",
      "Â»f attorney\n",
      "Pre\n",
      "v\n",
      "\n",
      "Q\n",
      "Q\n",
      "Y\n",
      "\n",
      "rorred upon John R. Evang\n",
      "2 eg pf Attorney dated Januay y Q,\n",
      "r , ar ch revocatl be\n",
      "1 _ of Janus paserns a a ang hy\n",
      "a rf Directors of thig \\\n",
      "Sy\n",
      "\n",
      "apard o\n",
      "\n",
      "and passed bY the f\n",
      "\n",
      "cnet John R. Evans Cit} Dy\n",
      ", _â€” p, 1957 by 808 pee , Beare.\n",
      "sq prior â€™Â° â€œâ€”\n",
      "...\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š PIPELINE RANKING:\n",
      "Rank   Pipeline                       Confidence   Chars   \n",
      "------------------------------------------------------------\n",
      "ðŸ‘‘      grayscale_only                 84.37        2200    \n",
      "2.     inverted_only                  83.81        2210    \n",
      "3.     dewarp_only                    81.84        2277    \n",
      "4.     gray_to_bw                     80.50        2217    \n",
      "5.     deskew_gray_bw                 80.24        2423    \n",
      "6.     inverted_gray_bw               76.95        1455    \n",
      "7.     dewarp_thick                   74.49        2284    \n",
      "8.     dewarp_thin                    73.14        2275    \n",
      "9.     inverted_gray_bw_no_noise      65.91        1780    \n",
      "10.    gray_bw_no_noise               46.60        772     \n",
      "11.    deskew_gray_bw_no_noise        45.25        1143    \n",
      "12.    gray_bw_no_noise_thick         43.04        1049    \n",
      "13.    gray_bw_no_noise_thin          42.83        604     \n",
      "\n",
      "ðŸ’¾ Saved best image: adaptive_ocr_results/BEST_page_2.png\n",
      "ðŸ’¾ Saved text: adaptive_ocr_results/page_2.txt\n",
      "\n",
      "============================================================\n",
      "FILE 2/2: page_1.png\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Testing 13 preprocessing pipelines on:\n",
      "  page_1.png\n",
      "============================================================\n",
      "\n",
      "[1/13] Testing: grayscale_only\n",
      "    Steps: grayscale\n",
      "    âœ“ Confidence: 100.86 | Chars: 3472\n",
      "\n",
      "[2/13] Testing: inverted_only\n",
      "    Steps: inverted\n",
      "    âœ“ Confidence: 101.06 | Chars: 3474\n",
      "\n",
      "[3/13] Testing: gray_to_bw\n",
      "    Steps: grayscale â†’ bw\n",
      "    âœ“ Confidence: 97.25 | Chars: 3624\n",
      "\n",
      "[4/13] Testing: gray_bw_no_noise\n",
      "    Steps: grayscale â†’ bw â†’ no_noise\n",
      "    âœ“ Confidence: 57.15 | Chars: 1217\n",
      "\n",
      "[5/13] Testing: gray_bw_no_noise_thin\n",
      "    Steps: grayscale â†’ bw â†’ no_noise â†’ thin\n",
      "    âœ“ Confidence: 60.29 | Chars: 1048\n",
      "\n",
      "[6/13] Testing: gray_bw_no_noise_thick\n",
      "    Steps: grayscale â†’ bw â†’ no_noise â†’ thick\n",
      "    âœ“ Confidence: 41.15 | Chars: 1907\n",
      "\n",
      "[7/13] Testing: deskew_gray_bw\n",
      "    Steps: deskew â†’ grayscale â†’ bw\n",
      "    âœ“ Confidence: 97.25 | Chars: 3624\n",
      "\n",
      "[8/13] Testing: deskew_gray_bw_no_noise\n",
      "    Steps: deskew â†’ grayscale â†’ bw â†’ no_noise\n",
      "    âœ“ Confidence: 57.15 | Chars: 1217\n",
      "\n",
      "[9/13] Testing: dewarp_only\n",
      "    Steps: dewarp\n",
      "Got 49 spans with 578 points.\n",
      "Initial objective is 0.006027032843382374\n",
      "Optimizing 635 parameters...\n",
      "Optimization took 2.28 sec.\n",
      "Final objective is 0.005907515343662956\n",
      "Got page dims 0.8053196793490193 x 1.8547748338918375\n",
      "Output will be 896x2048\n",
      "    âœ“ Confidence: 87.68 | Chars: 2815\n",
      "\n",
      "[10/13] Testing: dewarp_thin\n",
      "    Steps: dewarp â†’ thin\n",
      "Got 49 spans with 578 points.\n",
      "Initial objective is 0.006027032843382374\n",
      "Optimizing 635 parameters...\n",
      "Optimization took 2.29 sec.\n",
      "Final objective is 0.005907515343662956\n",
      "Got page dims 0.8053196793490193 x 1.8547748338918375\n",
      "Output will be 896x2048\n",
      "    âœ“ Confidence: 79.16 | Chars: 2796\n",
      "\n",
      "[11/13] Testing: dewarp_thick\n",
      "    Steps: dewarp â†’ thick\n",
      "Got 49 spans with 578 points.\n",
      "Initial objective is 0.006027032843382374\n",
      "Optimizing 635 parameters...\n",
      "Optimization took 2.31 sec.\n",
      "Final objective is 0.005907515343662956\n",
      "Got page dims 0.8053196793490193 x 1.8547748338918375\n",
      "Output will be 896x2048\n",
      "    âœ“ Confidence: 82.92 | Chars: 2794\n",
      "\n",
      "[12/13] Testing: inverted_gray_bw\n",
      "    Steps: inverted â†’ grayscale â†’ bw\n",
      "    âœ“ Confidence: 81.67 | Chars: 2575\n",
      "\n",
      "[13/13] Testing: inverted_gray_bw_no_noise\n",
      "    Steps: inverted â†’ grayscale â†’ bw â†’ no_noise\n",
      "    âœ“ Confidence: 72.67 | Chars: 3002\n",
      "\n",
      "\n",
      "============================================================\n",
      "ðŸ† BEST PIPELINE RESULTS\n",
      "============================================================\n",
      "Pipeline: inverted_only\n",
      "Steps: inverted\n",
      "Confidence Score: 101.06\n",
      "Characters Detected: 3474\n",
      "\n",
      "Extracted Text Preview (first 300 chars):\n",
      "------------------------------------------------------------\n",
      "KNOW ALL MEN BY THESE PRESENTS: That PAM AMERICAN\n",
      "\n",
      "aa Company) a Delaware corporation of Tulaa, Oklahoma,\n",
      "\n",
      "jc\n",
      "}\n",
      "\n",
      "| does hereby make, constitute and appoint JOHN A. EVANS and B. Vs HEWITT and WM, J. NOLTE of\n",
      "\n",
      "|\n",
      "\n",
      "lp .\n",
      "\n",
      "jFort Worth, Texas, or any of them, its true and lawful attorneys for and in its na\n",
      "...\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š PIPELINE RANKING:\n",
      "Rank   Pipeline                       Confidence   Chars   \n",
      "------------------------------------------------------------\n",
      "ðŸ‘‘      inverted_only                  101.06       3474    \n",
      "2.     grayscale_only                 100.86       3472    \n",
      "3.     gray_to_bw                     97.25        3624    \n",
      "4.     deskew_gray_bw                 97.25        3624    \n",
      "5.     dewarp_only                    87.68        2815    \n",
      "6.     dewarp_thick                   82.92        2794    \n",
      "7.     inverted_gray_bw               81.67        2575    \n",
      "8.     dewarp_thin                    79.16        2796    \n",
      "9.     inverted_gray_bw_no_noise      72.67        3002    \n",
      "10.    gray_bw_no_noise_thin          60.29        1048    \n",
      "11.    gray_bw_no_noise               57.15        1217    \n",
      "12.    deskew_gray_bw_no_noise        57.15        1217    \n",
      "13.    gray_bw_no_noise_thick         41.15        1907    \n",
      "\n",
      "ðŸ’¾ Saved best image: adaptive_ocr_results/BEST_page_1.png\n",
      "ðŸ’¾ Saved text: adaptive_ocr_results/page_1.txt\n",
      "\n",
      "############################################################\n",
      "# BATCH PROCESSING COMPLETE\n",
      "# Processed: 2/2 files\n",
      "# Results saved to: adaptive_ocr_results\n",
      "############################################################\n",
      "\n",
      "\n",
      "ðŸ“ˆ OVERALL STATISTICS:\n",
      "Average Confidence: 92.72\n",
      "Total Characters: 5674\n",
      "\n",
      "ðŸ“Š PIPELINE USAGE SUMMARY:\n",
      "  grayscale_only: 1 pages\n",
      "  inverted_only: 1 pages\n",
      "\n",
      "======================================================================\n",
      "STEP 2: DATA EXTRACTION WITH OLLAMA\n",
      "----------------------------------------------------------------------\n",
      "âœ… Ollama Data Extractor initialized with model: ocranalyst:latest\n",
      "\n",
      "======================================================================\n",
      "ðŸ“Š OLLAMA DATA EXTRACTION TO CSV\n",
      "======================================================================\n",
      "âŒ Ollama connection failed: Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download\n",
      "   Make sure Ollama is running: 'ollama serve'\n",
      "   And model is available: 'ollama list'\n",
      "âš ï¸  Data extraction failed, but OCR results are available\n",
      "   Check adaptive_ocr_results/ folder for text files\n",
      "âœ… Ollama Data Extractor initialized with model: ocranalyst:latest\n",
      "\n",
      "======================================================================\n",
      "ðŸ“Š OLLAMA DATA EXTRACTION TO CSV\n",
      "======================================================================\n",
      "âŒ Ollama connection failed: Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download\n",
      "   Make sure Ollama is running: 'ollama serve'\n",
      "   And model is available: 'ollama list'\n"
     ]
    }
   ],
   "source": [
    "# ========== MINIMAL MAIN EXECUTION ==========\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ðŸ“š DOCUMENT PREPROCESSING & ADAPTIVE OCR SYSTEM\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # Uncomment the function you want to run:\n",
    "    \n",
    "    # For first-time setup:\n",
    "    #PIL_to_png_save()\n",
    "    \n",
    "    # For quick testing:\n",
    "    # quick_test_on_single_page()\n",
    "\n",
    "    # ================ UNCOMMENT THIS BEFORE RUNNING COMMNADLINE ARGUMENT ================\n",
    "    # ***************** I used this to test this in the python IDE *****************\n",
    "    \n",
    "    # parser = argparse.ArgumentParser(\n",
    "    #     description=\"Document preprocessing + adaptive OCR + Specific contract data extraction\"\n",
    "    # )\n",
    "\n",
    "    # parser.add_argument(\n",
    "    #     \"input_path\",\n",
    "    #     type=str,\n",
    "    #     help=\"Only accepts PDF files\"\n",
    "    # )\n",
    "\n",
    "    # parser.add_argument(\n",
    "    #     \"--save\",\n",
    "    #     action=\"store_true\",\n",
    "    #     help=\"Save intermediary algorithm files\"\n",
    "    # )\n",
    "\n",
    "    # args = parser.parse_args()\n",
    "\n",
    "    # input_path = args.input_path\n",
    "    # save = args.save\n",
    "\n",
    "    input_path = \"./Scanned_0002-011.pdf\"\n",
    "    save = False\n",
    "    \n",
    "    PIL_to_png_save(input_path)\n",
    "    image_document_pre_processing(input_path, save)\n",
    "    qwen_data_extractor()\n",
    "\n",
    "\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d720cf-838a-4e7e-bbda-6947efb4d376",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52baeee-99f8-4428-97da-a5579b387057",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
